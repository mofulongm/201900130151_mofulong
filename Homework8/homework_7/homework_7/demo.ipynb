{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "import argparse\r\n",
    "import os\r\n",
    "import numpy as np\r\n",
    "import math\r\n",
    "\r\n",
    "import torchvision.transforms as transforms\r\n",
    "from torchvision.utils import save_image\r\n",
    "\r\n",
    "from torch.utils.data import DataLoader\r\n",
    "from torchvision import datasets\r\n",
    "from torch.autograd import Variable\r\n",
    "\r\n",
    "import torch.nn as nn\r\n",
    "import torch.nn.functional as F\r\n",
    "import torch\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "# Loss functions\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "\r\n",
    "\r\n",
    "parser = argparse.ArgumentParser()\r\n",
    "parser.add_argument(\"--n_epochs\", type=int, default=200, help=\"number of epochs of training\")\r\n",
    "parser.add_argument(\"--batch_size\", type=int, default=64, help=\"size of the batches\")\r\n",
    "parser.add_argument(\"--lr\", type=float, default=0.0002, help=\"adam: learning rate\")\r\n",
    "parser.add_argument(\"--b1\", type=float, default=0.5, help=\"adam: decay of first order momentum of gradient\")\r\n",
    "parser.add_argument(\"--b2\", type=float, default=0.999, help=\"adam: decay of first order momentum of gradient\")\r\n",
    "parser.add_argument(\"--n_cpu\", type=int, default=8, help=\"number of cpu threads to use during batch generation\")\r\n",
    "parser.add_argument(\"--latent_dim\", type=int, default=100, help=\"dimensionality of the latent space\")\r\n",
    "parser.add_argument(\"--n_classes\", type=int, default=10, help=\"number of classes for dataset\")\r\n",
    "parser.add_argument(\"--img_size\", type=int, default=32, help=\"size of each image dimension\")\r\n",
    "parser.add_argument(\"--channels\", type=int, default=1, help=\"number of image channels\")\r\n",
    "parser.add_argument(\"--sample_interval\", type=int, default=400, help=\"interval between image sampling\")\r\n",
    "opt = parser.parse_args(args=[])\r\n",
    "print(opt)\r\n",
    "\r\n",
    "img_shape = (opt.channels, opt.img_size, opt.img_size)\r\n",
    "\r\n",
    "cuda = True if torch.cuda.is_available() else False\r\n",
    "\r\n",
    "\r\n",
    "class Generator(nn.Module):\r\n",
    "    def __init__(self):\r\n",
    "        super(Generator, self).__init__()\r\n",
    "\r\n",
    "        self.label_emb = nn.Embedding(opt.n_classes, opt.n_classes)\r\n",
    "\r\n",
    "        def block(in_feat, out_feat, normalize=True):\r\n",
    "            layers = [nn.Linear(in_feat, out_feat)]\r\n",
    "            if normalize:\r\n",
    "                layers.append(nn.BatchNorm1d(out_feat, 0.8))\r\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\r\n",
    "            return layers\r\n",
    "\r\n",
    "        self.model = nn.Sequential(\r\n",
    "            *block(opt.latent_dim + opt.n_classes, 128, normalize=False),\r\n",
    "            *block(128, 256),\r\n",
    "            *block(256, 512),\r\n",
    "            *block(512, 1024),\r\n",
    "            nn.Linear(1024, int(np.prod(img_shape))),\r\n",
    "            nn.Tanh()\r\n",
    "        )\r\n",
    "\r\n",
    "    def forward(self, noise, labels):\r\n",
    "        # Concatenate label embedding and image to produce input\r\n",
    "        gen_input = torch.cat((self.label_emb(labels), noise), -1)\r\n",
    "        img = self.model(gen_input)\r\n",
    "        img = img.view(img.size(0), *img_shape)\r\n",
    "        return img\r\n",
    "\r\n",
    "\r\n",
    "class Discriminator(nn.Module):\r\n",
    "    def __init__(self):\r\n",
    "        super(Discriminator, self).__init__()\r\n",
    "\r\n",
    "        self.label_embedding = nn.Embedding(opt.n_classes, opt.n_classes)\r\n",
    "\r\n",
    "        self.model = nn.Sequential(\r\n",
    "            nn.Linear(opt.n_classes + int(np.prod(img_shape)), 512),\r\n",
    "            nn.LeakyReLU(0.2, inplace=True),\r\n",
    "            nn.Linear(512, 512),\r\n",
    "            nn.Dropout(0.4),\r\n",
    "            nn.LeakyReLU(0.2, inplace=True),\r\n",
    "            nn.Linear(512, 512),\r\n",
    "            nn.Dropout(0.4),\r\n",
    "            nn.LeakyReLU(0.2, inplace=True),\r\n",
    "            nn.Linear(512, 1),\r\n",
    "        )\r\n",
    "\r\n",
    "    def forward(self, img, labels):\r\n",
    "        # Concatenate label embedding and image to produce input\r\n",
    "        d_in = torch.cat((img.view(img.size(0), -1), self.label_embedding(labels)), -1)\r\n",
    "        validity = self.model(d_in)\r\n",
    "        return validity\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Namespace(b1=0.5, b2=0.999, batch_size=64, channels=1, img_size=32, latent_dim=100, lr=0.0002, n_classes=10, n_cpu=8, n_epochs=200, sample_interval=400)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "adversarial_loss = torch.nn.MSELoss()\r\n",
    "\r\n",
    "# Initialize generator and discriminator\r\n",
    "generator = Generator()\r\n",
    "discriminator = Discriminator()\r\n",
    "\r\n",
    "if cuda:\r\n",
    "    generator.cuda()\r\n",
    "    discriminator.cuda()\r\n",
    "    adversarial_loss.cuda()\r\n",
    "\r\n",
    "# Configure data loader\r\n",
    "os.makedirs(\"../../data/FashionMNIST\", exist_ok=True)\r\n",
    "dataloader = torch.utils.data.DataLoader(\r\n",
    "    datasets.MNIST(\r\n",
    "        \"../../data/FashionMNIST\",\r\n",
    "        train=True,\r\n",
    "        download=True,\r\n",
    "        transform=transforms.Compose(\r\n",
    "            [transforms.Resize(opt.img_size), transforms.ToTensor(), transforms.Normalize([0.5], [0.5])]\r\n",
    "        ),\r\n",
    "    ),\r\n",
    "    batch_size=opt.batch_size,\r\n",
    "    shuffle=True,\r\n",
    ")\r\n",
    "\r\n",
    "# Optimizers\r\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\r\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\r\n",
    "\r\n",
    "FloatTensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\r\n",
    "LongTensor = torch.cuda.LongTensor if cuda else torch.LongTensor\r\n",
    "\r\n",
    "\r\n",
    "def sample_image(n_row, batches_done):\r\n",
    "    \"\"\"Saves a grid of generated digits ranging from 0 to n_classes\"\"\"\r\n",
    "    # Sample noise\r\n",
    "    z = Variable(FloatTensor(np.random.normal(0, 1, (n_row ** 2, opt.latent_dim))))\r\n",
    "    # Get labels ranging from 0 to n_classes for n rows\r\n",
    "    labels = np.array([num for _ in range(n_row) for num in range(n_row)])\r\n",
    "    labels = Variable(LongTensor(labels))\r\n",
    "    gen_imgs = generator(z, labels)\r\n",
    "    save_image(gen_imgs.data, \"images/%d.png\" % batches_done, nrow=n_row, normalize=True)\r\n",
    "\r\n",
    "\r\n",
    "# ----------\r\n",
    "#  Training\r\n",
    "# ----------\r\n",
    "\r\n",
    "for epoch in range(opt.n_epochs):\r\n",
    "    for i, (imgs, labels) in enumerate(dataloader):\r\n",
    "\r\n",
    "        batch_size = imgs.shape[0]\r\n",
    "\r\n",
    "        # Adversarial ground truths\r\n",
    "        valid = Variable(FloatTensor(batch_size, 1).fill_(1.0), requires_grad=False)\r\n",
    "        fake = Variable(FloatTensor(batch_size, 1).fill_(0.0), requires_grad=False)\r\n",
    "\r\n",
    "        # Configure input\r\n",
    "        real_imgs = Variable(imgs.type(FloatTensor))\r\n",
    "        labels = Variable(labels.type(LongTensor))\r\n",
    "\r\n",
    "        # -----------------\r\n",
    "        #  Train Generator\r\n",
    "        # -----------------\r\n",
    "\r\n",
    "        optimizer_G.zero_grad()\r\n",
    "\r\n",
    "        # Sample noise and labels as generator input\r\n",
    "        z = Variable(FloatTensor(np.random.normal(0, 1, (batch_size, opt.latent_dim))))\r\n",
    "        gen_labels = Variable(LongTensor(np.random.randint(0, opt.n_classes, batch_size)))\r\n",
    "\r\n",
    "        # Generate a batch of images\r\n",
    "        gen_imgs = generator(z, gen_labels)\r\n",
    "\r\n",
    "        # Loss measures generator's ability to fool the discriminator\r\n",
    "        validity = discriminator(gen_imgs, gen_labels)\r\n",
    "        g_loss = adversarial_loss(validity, valid)\r\n",
    "\r\n",
    "        g_loss.backward()\r\n",
    "        optimizer_G.step()\r\n",
    "\r\n",
    "        # ---------------------\r\n",
    "        #  Train Discriminator\r\n",
    "        # ---------------------\r\n",
    "\r\n",
    "        optimizer_D.zero_grad()\r\n",
    "\r\n",
    "        # Loss for real images\r\n",
    "        validity_real = discriminator(real_imgs, labels)\r\n",
    "        d_real_loss = adversarial_loss(validity_real, valid)\r\n",
    "\r\n",
    "        # Loss for fake images\r\n",
    "        validity_fake = discriminator(gen_imgs.detach(), gen_labels)\r\n",
    "        d_fake_loss = adversarial_loss(validity_fake, fake)\r\n",
    "\r\n",
    "        # Total discriminator loss\r\n",
    "        d_loss = (d_real_loss + d_fake_loss) / 2\r\n",
    "\r\n",
    "        d_loss.backward()\r\n",
    "        optimizer_D.step()\r\n",
    "\r\n",
    "        print(\r\n",
    "            \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\"\r\n",
    "            % (epoch, opt.n_epochs, i, len(dataloader), d_loss.item(), g_loss.item())\r\n",
    "        )\r\n",
    "\r\n",
    "        batches_done = epoch * len(dataloader) + i\r\n",
    "        if batches_done % opt.sample_interval == 0:\r\n",
    "            sample_image(n_row=10, batches_done=batches_done)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../../data/FashionMNIST\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Extracting ../../data/FashionMNIST\\MNIST\\raw\\train-images-idx3-ubyte.gz to ../../data/FashionMNIST\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../../data/FashionMNIST\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "102.8%\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Extracting ../../data/FashionMNIST\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ../../data/FashionMNIST\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../../data/FashionMNIST\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Extracting ../../data/FashionMNIST\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ../../data/FashionMNIST\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../../data/FashionMNIST\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "112.7%\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Extracting ../../data/FashionMNIST\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ../../data/FashionMNIST\\MNIST\\raw\n",
      "\n",
      "[Epoch 0/200] [Batch 0/938] [D loss: 0.530964] [G loss: 1.037293]\n",
      "[Epoch 0/200] [Batch 1/938] [D loss: 0.307221] [G loss: 1.014155]\n",
      "[Epoch 0/200] [Batch 2/938] [D loss: 0.153182] [G loss: 0.990506]\n",
      "[Epoch 0/200] [Batch 3/938] [D loss: 0.025656] [G loss: 0.970634]\n",
      "[Epoch 0/200] [Batch 4/938] [D loss: 0.012895] [G loss: 0.950936]\n",
      "[Epoch 0/200] [Batch 5/938] [D loss: 0.041923] [G loss: 0.934435]\n",
      "[Epoch 0/200] [Batch 6/938] [D loss: 0.012425] [G loss: 0.937833]\n",
      "[Epoch 0/200] [Batch 7/938] [D loss: 0.014805] [G loss: 0.934551]\n",
      "[Epoch 0/200] [Batch 8/938] [D loss: 0.015529] [G loss: 0.921564]\n",
      "[Epoch 0/200] [Batch 9/938] [D loss: 0.009537] [G loss: 0.895007]\n",
      "[Epoch 0/200] [Batch 10/938] [D loss: 0.009007] [G loss: 0.888259]\n",
      "[Epoch 0/200] [Batch 11/938] [D loss: 0.012510] [G loss: 0.877678]\n",
      "[Epoch 0/200] [Batch 12/938] [D loss: 0.011648] [G loss: 0.863765]\n",
      "[Epoch 0/200] [Batch 13/938] [D loss: 0.011035] [G loss: 0.849214]\n",
      "[Epoch 0/200] [Batch 14/938] [D loss: 0.010030] [G loss: 0.832123]\n",
      "[Epoch 0/200] [Batch 15/938] [D loss: 0.010923] [G loss: 0.831545]\n",
      "[Epoch 0/200] [Batch 16/938] [D loss: 0.013887] [G loss: 0.814668]\n",
      "[Epoch 0/200] [Batch 17/938] [D loss: 0.015332] [G loss: 0.788965]\n",
      "[Epoch 0/200] [Batch 18/938] [D loss: 0.015772] [G loss: 0.775339]\n",
      "[Epoch 0/200] [Batch 19/938] [D loss: 0.016520] [G loss: 0.751213]\n",
      "[Epoch 0/200] [Batch 20/938] [D loss: 0.018939] [G loss: 0.739297]\n",
      "[Epoch 0/200] [Batch 21/938] [D loss: 0.019998] [G loss: 0.734805]\n",
      "[Epoch 0/200] [Batch 22/938] [D loss: 0.021277] [G loss: 0.711939]\n",
      "[Epoch 0/200] [Batch 23/938] [D loss: 0.019562] [G loss: 0.724220]\n",
      "[Epoch 0/200] [Batch 24/938] [D loss: 0.023592] [G loss: 0.735636]\n",
      "[Epoch 0/200] [Batch 25/938] [D loss: 0.018482] [G loss: 0.734385]\n",
      "[Epoch 0/200] [Batch 26/938] [D loss: 0.016809] [G loss: 0.721578]\n",
      "[Epoch 0/200] [Batch 27/938] [D loss: 0.017935] [G loss: 0.782127]\n",
      "[Epoch 0/200] [Batch 28/938] [D loss: 0.018007] [G loss: 0.804396]\n",
      "[Epoch 0/200] [Batch 29/938] [D loss: 0.015731] [G loss: 0.767270]\n",
      "[Epoch 0/200] [Batch 30/938] [D loss: 0.014262] [G loss: 0.864229]\n",
      "[Epoch 0/200] [Batch 31/938] [D loss: 0.011532] [G loss: 0.845890]\n",
      "[Epoch 0/200] [Batch 32/938] [D loss: 0.013980] [G loss: 0.827482]\n",
      "[Epoch 0/200] [Batch 33/938] [D loss: 0.022289] [G loss: 0.961434]\n",
      "[Epoch 0/200] [Batch 34/938] [D loss: 0.050580] [G loss: 0.642074]\n",
      "[Epoch 0/200] [Batch 35/938] [D loss: 0.056941] [G loss: 0.995066]\n",
      "[Epoch 0/200] [Batch 36/938] [D loss: 0.039290] [G loss: 0.571972]\n",
      "[Epoch 0/200] [Batch 37/938] [D loss: 0.036782] [G loss: 0.633884]\n",
      "[Epoch 0/200] [Batch 38/938] [D loss: 0.026082] [G loss: 0.812321]\n",
      "[Epoch 0/200] [Batch 39/938] [D loss: 0.020508] [G loss: 0.727426]\n",
      "[Epoch 0/200] [Batch 40/938] [D loss: 0.023648] [G loss: 0.854624]\n",
      "[Epoch 0/200] [Batch 41/938] [D loss: 0.029229] [G loss: 0.694859]\n",
      "[Epoch 0/200] [Batch 42/938] [D loss: 0.099741] [G loss: 1.094149]\n",
      "[Epoch 0/200] [Batch 43/938] [D loss: 0.294685] [G loss: 0.237323]\n",
      "[Epoch 0/200] [Batch 44/938] [D loss: 0.072584] [G loss: 0.652767]\n",
      "[Epoch 0/200] [Batch 45/938] [D loss: 0.115775] [G loss: 0.764936]\n",
      "[Epoch 0/200] [Batch 46/938] [D loss: 0.072749] [G loss: 0.642050]\n",
      "[Epoch 0/200] [Batch 47/938] [D loss: 0.058215] [G loss: 0.506364]\n",
      "[Epoch 0/200] [Batch 48/938] [D loss: 0.052147] [G loss: 0.553259]\n",
      "[Epoch 0/200] [Batch 49/938] [D loss: 0.037799] [G loss: 0.694604]\n",
      "[Epoch 0/200] [Batch 50/938] [D loss: 0.033582] [G loss: 0.770456]\n",
      "[Epoch 0/200] [Batch 51/938] [D loss: 0.040686] [G loss: 0.592136]\n",
      "[Epoch 0/200] [Batch 52/938] [D loss: 0.039593] [G loss: 0.811615]\n",
      "[Epoch 0/200] [Batch 53/938] [D loss: 0.081091] [G loss: 0.425132]\n",
      "[Epoch 0/200] [Batch 54/938] [D loss: 0.166722] [G loss: 1.118803]\n",
      "[Epoch 0/200] [Batch 55/938] [D loss: 0.328572] [G loss: 0.121576]\n",
      "[Epoch 0/200] [Batch 56/938] [D loss: 0.115983] [G loss: 0.468440]\n",
      "[Epoch 0/200] [Batch 57/938] [D loss: 0.164643] [G loss: 0.647404]\n",
      "[Epoch 0/200] [Batch 58/938] [D loss: 0.129865] [G loss: 0.490138]\n",
      "[Epoch 0/200] [Batch 59/938] [D loss: 0.116343] [G loss: 0.355244]\n",
      "[Epoch 0/200] [Batch 60/938] [D loss: 0.112755] [G loss: 0.373245]\n",
      "[Epoch 0/200] [Batch 61/938] [D loss: 0.110677] [G loss: 0.551290]\n",
      "[Epoch 0/200] [Batch 62/938] [D loss: 0.097836] [G loss: 0.442691]\n",
      "[Epoch 0/200] [Batch 63/938] [D loss: 0.110140] [G loss: 0.457595]\n",
      "[Epoch 0/200] [Batch 64/938] [D loss: 0.094300] [G loss: 0.510938]\n",
      "[Epoch 0/200] [Batch 65/938] [D loss: 0.100927] [G loss: 0.556822]\n",
      "[Epoch 0/200] [Batch 66/938] [D loss: 0.211456] [G loss: 0.192300]\n",
      "[Epoch 0/200] [Batch 67/938] [D loss: 0.662632] [G loss: 1.704281]\n",
      "[Epoch 0/200] [Batch 68/938] [D loss: 0.249144] [G loss: 0.112008]\n",
      "[Epoch 0/200] [Batch 69/938] [D loss: 0.177586] [G loss: 0.218680]\n",
      "[Epoch 0/200] [Batch 70/938] [D loss: 0.143348] [G loss: 0.528093]\n",
      "[Epoch 0/200] [Batch 71/938] [D loss: 0.126377] [G loss: 0.592553]\n",
      "[Epoch 0/200] [Batch 72/938] [D loss: 0.089380] [G loss: 0.400067]\n",
      "[Epoch 0/200] [Batch 73/938] [D loss: 0.054470] [G loss: 0.655357]\n",
      "[Epoch 0/200] [Batch 74/938] [D loss: 0.057409] [G loss: 0.720655]\n",
      "[Epoch 0/200] [Batch 75/938] [D loss: 0.083226] [G loss: 0.422583]\n",
      "[Epoch 0/200] [Batch 76/938] [D loss: 0.379242] [G loss: 1.851810]\n",
      "[Epoch 0/200] [Batch 77/938] [D loss: 1.152637] [G loss: 0.186736]\n",
      "[Epoch 0/200] [Batch 78/938] [D loss: 0.207276] [G loss: 0.509948]\n",
      "[Epoch 0/200] [Batch 79/938] [D loss: 0.303328] [G loss: 0.812842]\n",
      "[Epoch 0/200] [Batch 80/938] [D loss: 0.266460] [G loss: 0.669029]\n",
      "[Epoch 0/200] [Batch 81/938] [D loss: 0.216831] [G loss: 0.424570]\n",
      "[Epoch 0/200] [Batch 82/938] [D loss: 0.202812] [G loss: 0.304910]\n",
      "[Epoch 0/200] [Batch 83/938] [D loss: 0.192157] [G loss: 0.214923]\n",
      "[Epoch 0/200] [Batch 84/938] [D loss: 0.183870] [G loss: 0.271344]\n",
      "[Epoch 0/200] [Batch 85/938] [D loss: 0.166303] [G loss: 0.353157]\n",
      "[Epoch 0/200] [Batch 86/938] [D loss: 0.156288] [G loss: 0.430738]\n",
      "[Epoch 0/200] [Batch 87/938] [D loss: 0.143062] [G loss: 0.408076]\n",
      "[Epoch 0/200] [Batch 88/938] [D loss: 0.129974] [G loss: 0.391218]\n",
      "[Epoch 0/200] [Batch 89/938] [D loss: 0.117589] [G loss: 0.466441]\n",
      "[Epoch 0/200] [Batch 90/938] [D loss: 0.118493] [G loss: 0.468237]\n",
      "[Epoch 0/200] [Batch 91/938] [D loss: 0.117497] [G loss: 0.529677]\n",
      "[Epoch 0/200] [Batch 92/938] [D loss: 0.123215] [G loss: 0.323326]\n",
      "[Epoch 0/200] [Batch 93/938] [D loss: 0.280204] [G loss: 1.169473]\n",
      "[Epoch 0/200] [Batch 94/938] [D loss: 0.595240] [G loss: 0.024999]\n",
      "[Epoch 0/200] [Batch 95/938] [D loss: 0.188406] [G loss: 0.620306]\n",
      "[Epoch 0/200] [Batch 96/938] [D loss: 0.232022] [G loss: 0.693678]\n",
      "[Epoch 0/200] [Batch 97/938] [D loss: 0.176996] [G loss: 0.379981]\n",
      "[Epoch 0/200] [Batch 98/938] [D loss: 0.202729] [G loss: 0.212826]\n",
      "[Epoch 0/200] [Batch 99/938] [D loss: 0.176044] [G loss: 0.276938]\n",
      "[Epoch 0/200] [Batch 100/938] [D loss: 0.145022] [G loss: 0.498987]\n",
      "[Epoch 0/200] [Batch 101/938] [D loss: 0.152758] [G loss: 0.450629]\n",
      "[Epoch 0/200] [Batch 102/938] [D loss: 0.142605] [G loss: 0.322946]\n",
      "[Epoch 0/200] [Batch 103/938] [D loss: 0.124685] [G loss: 0.483808]\n",
      "[Epoch 0/200] [Batch 104/938] [D loss: 0.145418] [G loss: 0.493037]\n",
      "[Epoch 0/200] [Batch 105/938] [D loss: 0.120818] [G loss: 0.318445]\n",
      "[Epoch 0/200] [Batch 106/938] [D loss: 0.138648] [G loss: 0.495297]\n",
      "[Epoch 0/200] [Batch 107/938] [D loss: 0.154113] [G loss: 0.283606]\n",
      "[Epoch 0/200] [Batch 108/938] [D loss: 0.321221] [G loss: 1.079929]\n",
      "[Epoch 0/200] [Batch 109/938] [D loss: 0.700995] [G loss: 0.043556]\n",
      "[Epoch 0/200] [Batch 110/938] [D loss: 0.209085] [G loss: 0.472917]\n",
      "[Epoch 0/200] [Batch 111/938] [D loss: 0.266651] [G loss: 0.769174]\n",
      "[Epoch 0/200] [Batch 112/938] [D loss: 0.210153] [G loss: 0.493680]\n",
      "[Epoch 0/200] [Batch 113/938] [D loss: 0.182339] [G loss: 0.293900]\n",
      "[Epoch 0/200] [Batch 114/938] [D loss: 0.171650] [G loss: 0.313337]\n",
      "[Epoch 0/200] [Batch 115/938] [D loss: 0.145172] [G loss: 0.443933]\n",
      "[Epoch 0/200] [Batch 116/938] [D loss: 0.125605] [G loss: 0.524006]\n",
      "[Epoch 0/200] [Batch 117/938] [D loss: 0.105215] [G loss: 0.503443]\n",
      "[Epoch 0/200] [Batch 118/938] [D loss: 0.095472] [G loss: 0.484135]\n",
      "[Epoch 0/200] [Batch 119/938] [D loss: 0.088163] [G loss: 0.542572]\n",
      "[Epoch 0/200] [Batch 120/938] [D loss: 0.096113] [G loss: 0.624426]\n",
      "[Epoch 0/200] [Batch 121/938] [D loss: 0.125637] [G loss: 0.365020]\n",
      "[Epoch 0/200] [Batch 122/938] [D loss: 0.182730] [G loss: 1.007960]\n",
      "[Epoch 0/200] [Batch 123/938] [D loss: 0.409068] [G loss: 0.041050]\n",
      "[Epoch 0/200] [Batch 124/938] [D loss: 0.253775] [G loss: 1.036250]\n",
      "[Epoch 0/200] [Batch 125/938] [D loss: 0.148023] [G loss: 0.432971]\n",
      "[Epoch 0/200] [Batch 126/938] [D loss: 0.182489] [G loss: 0.221443]\n",
      "[Epoch 0/200] [Batch 127/938] [D loss: 0.120842] [G loss: 0.432118]\n",
      "[Epoch 0/200] [Batch 128/938] [D loss: 0.127762] [G loss: 0.581886]\n",
      "[Epoch 0/200] [Batch 129/938] [D loss: 0.123564] [G loss: 0.365539]\n",
      "[Epoch 0/200] [Batch 130/938] [D loss: 0.125101] [G loss: 0.487513]\n",
      "[Epoch 0/200] [Batch 131/938] [D loss: 0.118982] [G loss: 0.638878]\n",
      "[Epoch 0/200] [Batch 132/938] [D loss: 0.138440] [G loss: 0.323100]\n",
      "[Epoch 0/200] [Batch 133/938] [D loss: 0.150470] [G loss: 0.829476]\n",
      "[Epoch 0/200] [Batch 134/938] [D loss: 0.260937] [G loss: 0.119248]\n",
      "[Epoch 0/200] [Batch 135/938] [D loss: 0.362219] [G loss: 1.364118]\n",
      "[Epoch 0/200] [Batch 136/938] [D loss: 0.247571] [G loss: 0.083541]\n",
      "[Epoch 0/200] [Batch 137/938] [D loss: 0.177292] [G loss: 0.261262]\n",
      "[Epoch 0/200] [Batch 138/938] [D loss: 0.143535] [G loss: 0.646765]\n",
      "[Epoch 0/200] [Batch 139/938] [D loss: 0.114771] [G loss: 0.685677]\n",
      "[Epoch 0/200] [Batch 140/938] [D loss: 0.096113] [G loss: 0.458944]\n",
      "[Epoch 0/200] [Batch 141/938] [D loss: 0.082428] [G loss: 0.463074]\n",
      "[Epoch 0/200] [Batch 142/938] [D loss: 0.074828] [G loss: 0.727874]\n",
      "[Epoch 0/200] [Batch 143/938] [D loss: 0.074044] [G loss: 0.539381]\n",
      "[Epoch 0/200] [Batch 144/938] [D loss: 0.081078] [G loss: 0.502097]\n",
      "[Epoch 0/200] [Batch 145/938] [D loss: 0.100430] [G loss: 0.553380]\n",
      "[Epoch 0/200] [Batch 146/938] [D loss: 0.117237] [G loss: 0.482005]\n",
      "[Epoch 0/200] [Batch 147/938] [D loss: 0.124159] [G loss: 0.460068]\n",
      "[Epoch 0/200] [Batch 148/938] [D loss: 0.154198] [G loss: 0.595101]\n",
      "[Epoch 0/200] [Batch 149/938] [D loss: 0.242655] [G loss: 0.125891]\n",
      "[Epoch 0/200] [Batch 150/938] [D loss: 0.489236] [G loss: 1.483206]\n",
      "[Epoch 0/200] [Batch 151/938] [D loss: 0.358746] [G loss: 0.043188]\n",
      "[Epoch 0/200] [Batch 152/938] [D loss: 0.179022] [G loss: 0.228883]\n",
      "[Epoch 0/200] [Batch 153/938] [D loss: 0.145106] [G loss: 0.600997]\n",
      "[Epoch 0/200] [Batch 154/938] [D loss: 0.143034] [G loss: 0.677639]\n",
      "[Epoch 0/200] [Batch 155/938] [D loss: 0.105073] [G loss: 0.573112]\n",
      "[Epoch 0/200] [Batch 156/938] [D loss: 0.102363] [G loss: 0.422936]\n",
      "[Epoch 0/200] [Batch 157/938] [D loss: 0.082713] [G loss: 0.531265]\n",
      "[Epoch 0/200] [Batch 158/938] [D loss: 0.067348] [G loss: 0.641092]\n",
      "[Epoch 0/200] [Batch 159/938] [D loss: 0.070915] [G loss: 0.597420]\n",
      "[Epoch 0/200] [Batch 160/938] [D loss: 0.075900] [G loss: 0.494123]\n",
      "[Epoch 0/200] [Batch 161/938] [D loss: 0.087604] [G loss: 0.623017]\n",
      "[Epoch 0/200] [Batch 162/938] [D loss: 0.094380] [G loss: 0.481675]\n",
      "[Epoch 0/200] [Batch 163/938] [D loss: 0.112674] [G loss: 0.548770]\n",
      "[Epoch 0/200] [Batch 164/938] [D loss: 0.133814] [G loss: 0.341879]\n",
      "[Epoch 0/200] [Batch 165/938] [D loss: 0.136433] [G loss: 0.508652]\n",
      "[Epoch 0/200] [Batch 166/938] [D loss: 0.131526] [G loss: 0.340837]\n",
      "[Epoch 0/200] [Batch 167/938] [D loss: 0.132792] [G loss: 0.551020]\n",
      "[Epoch 0/200] [Batch 168/938] [D loss: 0.133548] [G loss: 0.310420]\n",
      "[Epoch 0/200] [Batch 169/938] [D loss: 0.139524] [G loss: 0.681764]\n",
      "[Epoch 0/200] [Batch 170/938] [D loss: 0.143587] [G loss: 0.276455]\n",
      "[Epoch 0/200] [Batch 171/938] [D loss: 0.160527] [G loss: 0.832786]\n",
      "[Epoch 0/200] [Batch 172/938] [D loss: 0.144909] [G loss: 0.261979]\n",
      "[Epoch 0/200] [Batch 173/938] [D loss: 0.111460] [G loss: 0.637481]\n",
      "[Epoch 0/200] [Batch 174/938] [D loss: 0.114219] [G loss: 0.426291]\n",
      "[Epoch 0/200] [Batch 175/938] [D loss: 0.125543] [G loss: 0.371676]\n",
      "[Epoch 0/200] [Batch 176/938] [D loss: 0.137189] [G loss: 0.578169]\n",
      "[Epoch 0/200] [Batch 177/938] [D loss: 0.171939] [G loss: 0.239271]\n",
      "[Epoch 0/200] [Batch 178/938] [D loss: 0.148861] [G loss: 0.618048]\n",
      "[Epoch 0/200] [Batch 179/938] [D loss: 0.146102] [G loss: 0.294853]\n",
      "[Epoch 0/200] [Batch 180/938] [D loss: 0.116910] [G loss: 0.510209]\n",
      "[Epoch 0/200] [Batch 181/938] [D loss: 0.088391] [G loss: 0.543182]\n",
      "[Epoch 0/200] [Batch 182/938] [D loss: 0.089699] [G loss: 0.485029]\n",
      "[Epoch 0/200] [Batch 183/938] [D loss: 0.097538] [G loss: 0.699414]\n",
      "[Epoch 0/200] [Batch 184/938] [D loss: 0.167187] [G loss: 0.225962]\n",
      "[Epoch 0/200] [Batch 185/938] [D loss: 0.383792] [G loss: 1.565436]\n",
      "[Epoch 0/200] [Batch 186/938] [D loss: 0.243192] [G loss: 0.145512]\n",
      "[Epoch 0/200] [Batch 187/938] [D loss: 0.128585] [G loss: 0.627855]\n",
      "[Epoch 0/200] [Batch 188/938] [D loss: 0.094574] [G loss: 0.684348]\n",
      "[Epoch 0/200] [Batch 189/938] [D loss: 0.084934] [G loss: 0.488159]\n",
      "[Epoch 0/200] [Batch 190/938] [D loss: 0.073650] [G loss: 0.487801]\n",
      "[Epoch 0/200] [Batch 191/938] [D loss: 0.067002] [G loss: 0.612523]\n",
      "[Epoch 0/200] [Batch 192/938] [D loss: 0.081781] [G loss: 0.580643]\n",
      "[Epoch 0/200] [Batch 193/938] [D loss: 0.097838] [G loss: 0.540020]\n",
      "[Epoch 0/200] [Batch 194/938] [D loss: 0.105826] [G loss: 0.506149]\n",
      "[Epoch 0/200] [Batch 195/938] [D loss: 0.114294] [G loss: 0.452365]\n",
      "[Epoch 0/200] [Batch 196/938] [D loss: 0.121824] [G loss: 0.803194]\n",
      "[Epoch 0/200] [Batch 197/938] [D loss: 0.260924] [G loss: 0.136295]\n",
      "[Epoch 0/200] [Batch 198/938] [D loss: 0.402328] [G loss: 1.608831]\n",
      "[Epoch 0/200] [Batch 199/938] [D loss: 0.138824] [G loss: 0.278586]\n",
      "[Epoch 0/200] [Batch 200/938] [D loss: 0.122670] [G loss: 0.305614]\n",
      "[Epoch 0/200] [Batch 201/938] [D loss: 0.097663] [G loss: 0.594465]\n",
      "[Epoch 0/200] [Batch 202/938] [D loss: 0.086950] [G loss: 0.649015]\n",
      "[Epoch 0/200] [Batch 203/938] [D loss: 0.098558] [G loss: 0.472721]\n",
      "[Epoch 0/200] [Batch 204/938] [D loss: 0.087473] [G loss: 0.509486]\n",
      "[Epoch 0/200] [Batch 205/938] [D loss: 0.108062] [G loss: 0.582298]\n",
      "[Epoch 0/200] [Batch 206/938] [D loss: 0.105118] [G loss: 0.435216]\n",
      "[Epoch 0/200] [Batch 207/938] [D loss: 0.114792] [G loss: 0.441585]\n",
      "[Epoch 0/200] [Batch 208/938] [D loss: 0.126343] [G loss: 0.435173]\n",
      "[Epoch 0/200] [Batch 209/938] [D loss: 0.122604] [G loss: 0.454014]\n",
      "[Epoch 0/200] [Batch 210/938] [D loss: 0.111963] [G loss: 0.389953]\n",
      "[Epoch 0/200] [Batch 211/938] [D loss: 0.118382] [G loss: 0.503760]\n",
      "[Epoch 0/200] [Batch 212/938] [D loss: 0.117364] [G loss: 0.392952]\n",
      "[Epoch 0/200] [Batch 213/938] [D loss: 0.123841] [G loss: 0.770322]\n",
      "[Epoch 0/200] [Batch 214/938] [D loss: 0.152458] [G loss: 0.268120]\n",
      "[Epoch 0/200] [Batch 215/938] [D loss: 0.152724] [G loss: 0.986167]\n",
      "[Epoch 0/200] [Batch 216/938] [D loss: 0.106053] [G loss: 0.422310]\n",
      "[Epoch 0/200] [Batch 217/938] [D loss: 0.087720] [G loss: 0.473460]\n",
      "[Epoch 0/200] [Batch 218/938] [D loss: 0.076898] [G loss: 0.833065]\n",
      "[Epoch 0/200] [Batch 219/938] [D loss: 0.093229] [G loss: 0.394002]\n",
      "[Epoch 0/200] [Batch 220/938] [D loss: 0.087905] [G loss: 0.712373]\n",
      "[Epoch 0/200] [Batch 221/938] [D loss: 0.125270] [G loss: 0.371939]\n",
      "[Epoch 0/200] [Batch 222/938] [D loss: 0.123767] [G loss: 0.822753]\n",
      "[Epoch 0/200] [Batch 223/938] [D loss: 0.211570] [G loss: 0.161686]\n",
      "[Epoch 0/200] [Batch 224/938] [D loss: 0.244612] [G loss: 1.293475]\n",
      "[Epoch 0/200] [Batch 225/938] [D loss: 0.173203] [G loss: 0.188996]\n",
      "[Epoch 0/200] [Batch 226/938] [D loss: 0.115846] [G loss: 0.356746]\n",
      "[Epoch 0/200] [Batch 227/938] [D loss: 0.117203] [G loss: 0.739621]\n",
      "[Epoch 0/200] [Batch 228/938] [D loss: 0.095080] [G loss: 0.522121]\n",
      "[Epoch 0/200] [Batch 229/938] [D loss: 0.089427] [G loss: 0.545475]\n",
      "[Epoch 0/200] [Batch 230/938] [D loss: 0.078493] [G loss: 0.796637]\n",
      "[Epoch 0/200] [Batch 231/938] [D loss: 0.127025] [G loss: 0.362898]\n",
      "[Epoch 0/200] [Batch 232/938] [D loss: 0.223562] [G loss: 1.322516]\n",
      "[Epoch 0/200] [Batch 233/938] [D loss: 0.732610] [G loss: 0.042317]\n",
      "[Epoch 0/200] [Batch 234/938] [D loss: 0.175973] [G loss: 0.659437]\n",
      "[Epoch 0/200] [Batch 235/938] [D loss: 0.244479] [G loss: 0.817438]\n",
      "[Epoch 0/200] [Batch 236/938] [D loss: 0.203065] [G loss: 0.577593]\n",
      "[Epoch 0/200] [Batch 237/938] [D loss: 0.178939] [G loss: 0.377934]\n",
      "[Epoch 0/200] [Batch 238/938] [D loss: 0.183928] [G loss: 0.288984]\n",
      "[Epoch 0/200] [Batch 239/938] [D loss: 0.165440] [G loss: 0.319313]\n",
      "[Epoch 0/200] [Batch 240/938] [D loss: 0.135441] [G loss: 0.426568]\n",
      "[Epoch 0/200] [Batch 241/938] [D loss: 0.142067] [G loss: 0.529890]\n",
      "[Epoch 0/200] [Batch 242/938] [D loss: 0.129718] [G loss: 0.461697]\n",
      "[Epoch 0/200] [Batch 243/938] [D loss: 0.126935] [G loss: 0.450608]\n",
      "[Epoch 0/200] [Batch 244/938] [D loss: 0.112599] [G loss: 0.481865]\n",
      "[Epoch 0/200] [Batch 245/938] [D loss: 0.116815] [G loss: 0.559705]\n",
      "[Epoch 0/200] [Batch 246/938] [D loss: 0.113927] [G loss: 0.451307]\n",
      "[Epoch 0/200] [Batch 247/938] [D loss: 0.114370] [G loss: 0.467041]\n",
      "[Epoch 0/200] [Batch 248/938] [D loss: 0.104365] [G loss: 0.606422]\n",
      "[Epoch 0/200] [Batch 249/938] [D loss: 0.119922] [G loss: 0.436608]\n",
      "[Epoch 0/200] [Batch 250/938] [D loss: 0.111376] [G loss: 0.447687]\n",
      "[Epoch 0/200] [Batch 251/938] [D loss: 0.150812] [G loss: 0.636586]\n",
      "[Epoch 0/200] [Batch 252/938] [D loss: 0.211437] [G loss: 0.170669]\n",
      "[Epoch 0/200] [Batch 253/938] [D loss: 0.187291] [G loss: 0.840460]\n",
      "[Epoch 0/200] [Batch 254/938] [D loss: 0.159007] [G loss: 0.299139]\n",
      "[Epoch 0/200] [Batch 255/938] [D loss: 0.138084] [G loss: 0.399443]\n",
      "[Epoch 0/200] [Batch 256/938] [D loss: 0.143235] [G loss: 0.524179]\n",
      "[Epoch 0/200] [Batch 257/938] [D loss: 0.123512] [G loss: 0.418421]\n",
      "[Epoch 0/200] [Batch 258/938] [D loss: 0.132730] [G loss: 0.392391]\n",
      "[Epoch 0/200] [Batch 259/938] [D loss: 0.141620] [G loss: 0.539286]\n",
      "[Epoch 0/200] [Batch 260/938] [D loss: 0.133795] [G loss: 0.357761]\n",
      "[Epoch 0/200] [Batch 261/938] [D loss: 0.121091] [G loss: 0.548404]\n",
      "[Epoch 0/200] [Batch 262/938] [D loss: 0.125213] [G loss: 0.496740]\n",
      "[Epoch 0/200] [Batch 263/938] [D loss: 0.164484] [G loss: 0.285201]\n",
      "[Epoch 0/200] [Batch 264/938] [D loss: 0.168929] [G loss: 0.770800]\n",
      "[Epoch 0/200] [Batch 265/938] [D loss: 0.188121] [G loss: 0.212642]\n",
      "[Epoch 0/200] [Batch 266/938] [D loss: 0.111737] [G loss: 0.689590]\n",
      "[Epoch 0/200] [Batch 267/938] [D loss: 0.109995] [G loss: 0.558819]\n",
      "[Epoch 0/200] [Batch 268/938] [D loss: 0.108433] [G loss: 0.439822]\n",
      "[Epoch 0/200] [Batch 269/938] [D loss: 0.098505] [G loss: 0.607699]\n",
      "[Epoch 0/200] [Batch 270/938] [D loss: 0.092688] [G loss: 0.539999]\n",
      "[Epoch 0/200] [Batch 271/938] [D loss: 0.098032] [G loss: 0.470765]\n",
      "[Epoch 0/200] [Batch 272/938] [D loss: 0.104404] [G loss: 0.769158]\n",
      "[Epoch 0/200] [Batch 273/938] [D loss: 0.182122] [G loss: 0.200450]\n",
      "[Epoch 0/200] [Batch 274/938] [D loss: 0.217299] [G loss: 1.150995]\n",
      "[Epoch 0/200] [Batch 275/938] [D loss: 0.321278] [G loss: 0.067475]\n",
      "[Epoch 0/200] [Batch 276/938] [D loss: 0.166761] [G loss: 0.701223]\n",
      "[Epoch 0/200] [Batch 277/938] [D loss: 0.163298] [G loss: 0.611706]\n",
      "[Epoch 0/200] [Batch 278/938] [D loss: 0.149465] [G loss: 0.350595]\n",
      "[Epoch 0/200] [Batch 279/938] [D loss: 0.140365] [G loss: 0.379683]\n",
      "[Epoch 0/200] [Batch 280/938] [D loss: 0.134067] [G loss: 0.467464]\n",
      "[Epoch 0/200] [Batch 281/938] [D loss: 0.122380] [G loss: 0.566022]\n",
      "[Epoch 0/200] [Batch 282/938] [D loss: 0.124266] [G loss: 0.411103]\n",
      "[Epoch 0/200] [Batch 283/938] [D loss: 0.102356] [G loss: 0.514487]\n",
      "[Epoch 0/200] [Batch 284/938] [D loss: 0.103969] [G loss: 0.546626]\n",
      "[Epoch 0/200] [Batch 285/938] [D loss: 0.104699] [G loss: 0.578462]\n",
      "[Epoch 0/200] [Batch 286/938] [D loss: 0.111260] [G loss: 0.471861]\n",
      "[Epoch 0/200] [Batch 287/938] [D loss: 0.119030] [G loss: 0.552343]\n",
      "[Epoch 0/200] [Batch 288/938] [D loss: 0.119910] [G loss: 0.453619]\n",
      "[Epoch 0/200] [Batch 289/938] [D loss: 0.124857] [G loss: 0.666845]\n",
      "[Epoch 0/200] [Batch 290/938] [D loss: 0.186918] [G loss: 0.228626]\n",
      "[Epoch 0/200] [Batch 291/938] [D loss: 0.224416] [G loss: 1.032992]\n",
      "[Epoch 0/200] [Batch 292/938] [D loss: 0.178708] [G loss: 0.223343]\n",
      "[Epoch 0/200] [Batch 293/938] [D loss: 0.123977] [G loss: 0.374222]\n",
      "[Epoch 0/200] [Batch 294/938] [D loss: 0.117584] [G loss: 0.725498]\n",
      "[Epoch 0/200] [Batch 295/938] [D loss: 0.119772] [G loss: 0.582267]\n",
      "[Epoch 0/200] [Batch 296/938] [D loss: 0.107393] [G loss: 0.383979]\n",
      "[Epoch 0/200] [Batch 297/938] [D loss: 0.093493] [G loss: 0.591747]\n",
      "[Epoch 0/200] [Batch 298/938] [D loss: 0.077496] [G loss: 0.626826]\n",
      "[Epoch 0/200] [Batch 299/938] [D loss: 0.076889] [G loss: 0.592447]\n",
      "[Epoch 0/200] [Batch 300/938] [D loss: 0.090369] [G loss: 0.495681]\n",
      "[Epoch 0/200] [Batch 301/938] [D loss: 0.095242] [G loss: 0.590260]\n",
      "[Epoch 0/200] [Batch 302/938] [D loss: 0.091225] [G loss: 0.558192]\n",
      "[Epoch 0/200] [Batch 303/938] [D loss: 0.116338] [G loss: 0.619393]\n",
      "[Epoch 0/200] [Batch 304/938] [D loss: 0.139301] [G loss: 0.401214]\n",
      "[Epoch 0/200] [Batch 305/938] [D loss: 0.250212] [G loss: 1.122555]\n",
      "[Epoch 0/200] [Batch 306/938] [D loss: 0.786418] [G loss: 0.058372]\n",
      "[Epoch 0/200] [Batch 307/938] [D loss: 0.185189] [G loss: 0.727557]\n",
      "[Epoch 0/200] [Batch 308/938] [D loss: 0.276672] [G loss: 0.939297]\n",
      "[Epoch 0/200] [Batch 309/938] [D loss: 0.190274] [G loss: 0.597283]\n",
      "[Epoch 0/200] [Batch 310/938] [D loss: 0.171702] [G loss: 0.356180]\n",
      "[Epoch 0/200] [Batch 311/938] [D loss: 0.162443] [G loss: 0.286797]\n",
      "[Epoch 0/200] [Batch 312/938] [D loss: 0.146071] [G loss: 0.339604]\n",
      "[Epoch 0/200] [Batch 313/938] [D loss: 0.126958] [G loss: 0.476038]\n",
      "[Epoch 0/200] [Batch 314/938] [D loss: 0.108861] [G loss: 0.559089]\n",
      "[Epoch 0/200] [Batch 315/938] [D loss: 0.109436] [G loss: 0.507791]\n",
      "[Epoch 0/200] [Batch 316/938] [D loss: 0.114470] [G loss: 0.465230]\n",
      "[Epoch 0/200] [Batch 317/938] [D loss: 0.100726] [G loss: 0.517987]\n",
      "[Epoch 0/200] [Batch 318/938] [D loss: 0.112370] [G loss: 0.518613]\n",
      "[Epoch 0/200] [Batch 319/938] [D loss: 0.092274] [G loss: 0.559682]\n",
      "[Epoch 0/200] [Batch 320/938] [D loss: 0.089715] [G loss: 0.520336]\n",
      "[Epoch 0/200] [Batch 321/938] [D loss: 0.103323] [G loss: 0.515092]\n",
      "[Epoch 0/200] [Batch 322/938] [D loss: 0.097979] [G loss: 0.571320]\n",
      "[Epoch 0/200] [Batch 323/938] [D loss: 0.108622] [G loss: 0.466188]\n",
      "[Epoch 0/200] [Batch 324/938] [D loss: 0.109338] [G loss: 0.624728]\n",
      "[Epoch 0/200] [Batch 325/938] [D loss: 0.133548] [G loss: 0.347369]\n",
      "[Epoch 0/200] [Batch 326/938] [D loss: 0.138519] [G loss: 0.728294]\n",
      "[Epoch 0/200] [Batch 327/938] [D loss: 0.169595] [G loss: 0.264384]\n",
      "[Epoch 0/200] [Batch 328/938] [D loss: 0.176777] [G loss: 0.903803]\n",
      "[Epoch 0/200] [Batch 329/938] [D loss: 0.205444] [G loss: 0.195914]\n",
      "[Epoch 0/200] [Batch 330/938] [D loss: 0.148505] [G loss: 0.553681]\n",
      "[Epoch 0/200] [Batch 331/938] [D loss: 0.112051] [G loss: 0.578248]\n",
      "[Epoch 0/200] [Batch 332/938] [D loss: 0.150063] [G loss: 0.363907]\n",
      "[Epoch 0/200] [Batch 333/938] [D loss: 0.148198] [G loss: 0.446509]\n",
      "[Epoch 0/200] [Batch 334/938] [D loss: 0.144248] [G loss: 0.703753]\n",
      "[Epoch 0/200] [Batch 335/938] [D loss: 0.164110] [G loss: 0.335966]\n",
      "[Epoch 0/200] [Batch 336/938] [D loss: 0.139725] [G loss: 0.690995]\n",
      "[Epoch 0/200] [Batch 337/938] [D loss: 0.118929] [G loss: 0.396429]\n",
      "[Epoch 0/200] [Batch 338/938] [D loss: 0.152844] [G loss: 0.670895]\n",
      "[Epoch 0/200] [Batch 339/938] [D loss: 0.189570] [G loss: 0.252032]\n",
      "[Epoch 0/200] [Batch 340/938] [D loss: 0.126177] [G loss: 0.807846]\n",
      "[Epoch 0/200] [Batch 341/938] [D loss: 0.123623] [G loss: 0.396205]\n",
      "[Epoch 0/200] [Batch 342/938] [D loss: 0.103201] [G loss: 0.606187]\n",
      "[Epoch 0/200] [Batch 343/938] [D loss: 0.091296] [G loss: 0.643665]\n",
      "[Epoch 0/200] [Batch 344/938] [D loss: 0.111512] [G loss: 0.489908]\n",
      "[Epoch 0/200] [Batch 345/938] [D loss: 0.088561] [G loss: 0.712200]\n",
      "[Epoch 0/200] [Batch 346/938] [D loss: 0.150298] [G loss: 0.292495]\n",
      "[Epoch 0/200] [Batch 347/938] [D loss: 0.250369] [G loss: 1.231920]\n",
      "[Epoch 0/200] [Batch 348/938] [D loss: 0.525321] [G loss: 0.026945]\n",
      "[Epoch 0/200] [Batch 349/938] [D loss: 0.184111] [G loss: 0.731089]\n",
      "[Epoch 0/200] [Batch 350/938] [D loss: 0.213245] [G loss: 0.761062]\n",
      "[Epoch 0/200] [Batch 351/938] [D loss: 0.147360] [G loss: 0.388020]\n",
      "[Epoch 0/200] [Batch 352/938] [D loss: 0.155132] [G loss: 0.309183]\n",
      "[Epoch 0/200] [Batch 353/938] [D loss: 0.121686] [G loss: 0.388569]\n",
      "[Epoch 0/200] [Batch 354/938] [D loss: 0.119275] [G loss: 0.590338]\n",
      "[Epoch 0/200] [Batch 355/938] [D loss: 0.108005] [G loss: 0.584958]\n",
      "[Epoch 0/200] [Batch 356/938] [D loss: 0.104026] [G loss: 0.456560]\n",
      "[Epoch 0/200] [Batch 357/938] [D loss: 0.102964] [G loss: 0.535462]\n",
      "[Epoch 0/200] [Batch 358/938] [D loss: 0.096332] [G loss: 0.564042]\n",
      "[Epoch 0/200] [Batch 359/938] [D loss: 0.099440] [G loss: 0.537143]\n",
      "[Epoch 0/200] [Batch 360/938] [D loss: 0.098416] [G loss: 0.485154]\n",
      "[Epoch 0/200] [Batch 361/938] [D loss: 0.103178] [G loss: 0.620730]\n",
      "[Epoch 0/200] [Batch 362/938] [D loss: 0.105084] [G loss: 0.491389]\n",
      "[Epoch 0/200] [Batch 363/938] [D loss: 0.113640] [G loss: 0.501927]\n",
      "[Epoch 0/200] [Batch 364/938] [D loss: 0.121793] [G loss: 0.652346]\n",
      "[Epoch 0/200] [Batch 365/938] [D loss: 0.133084] [G loss: 0.375932]\n",
      "[Epoch 0/200] [Batch 366/938] [D loss: 0.188506] [G loss: 1.003828]\n",
      "[Epoch 0/200] [Batch 367/938] [D loss: 0.357448] [G loss: 0.072709]\n",
      "[Epoch 0/200] [Batch 368/938] [D loss: 0.225487] [G loss: 1.115018]\n",
      "[Epoch 0/200] [Batch 369/938] [D loss: 0.124766] [G loss: 0.639063]\n",
      "[Epoch 0/200] [Batch 370/938] [D loss: 0.141832] [G loss: 0.285940]\n",
      "[Epoch 0/200] [Batch 371/938] [D loss: 0.092261] [G loss: 0.466357]\n",
      "[Epoch 0/200] [Batch 372/938] [D loss: 0.097303] [G loss: 0.671032]\n",
      "[Epoch 0/200] [Batch 373/938] [D loss: 0.081827] [G loss: 0.640080]\n",
      "[Epoch 0/200] [Batch 374/938] [D loss: 0.088694] [G loss: 0.510658]\n",
      "[Epoch 0/200] [Batch 375/938] [D loss: 0.074179] [G loss: 0.662657]\n",
      "[Epoch 0/200] [Batch 376/938] [D loss: 0.086523] [G loss: 0.571089]\n",
      "[Epoch 0/200] [Batch 377/938] [D loss: 0.083895] [G loss: 0.614416]\n",
      "[Epoch 0/200] [Batch 378/938] [D loss: 0.088846] [G loss: 0.587301]\n",
      "[Epoch 0/200] [Batch 379/938] [D loss: 0.095430] [G loss: 0.577804]\n",
      "[Epoch 0/200] [Batch 380/938] [D loss: 0.085311] [G loss: 0.574444]\n",
      "[Epoch 0/200] [Batch 381/938] [D loss: 0.108856] [G loss: 0.550190]\n",
      "[Epoch 0/200] [Batch 382/938] [D loss: 0.110185] [G loss: 0.624587]\n",
      "[Epoch 0/200] [Batch 383/938] [D loss: 0.118300] [G loss: 0.370880]\n",
      "[Epoch 0/200] [Batch 384/938] [D loss: 0.138857] [G loss: 1.067345]\n",
      "[Epoch 0/200] [Batch 385/938] [D loss: 0.329553] [G loss: 0.076793]\n",
      "[Epoch 0/200] [Batch 386/938] [D loss: 0.443590] [G loss: 1.777040]\n",
      "[Epoch 0/200] [Batch 387/938] [D loss: 0.174129] [G loss: 0.241306]\n",
      "[Epoch 0/200] [Batch 388/938] [D loss: 0.159373] [G loss: 0.233564]\n",
      "[Epoch 0/200] [Batch 389/938] [D loss: 0.127061] [G loss: 0.518361]\n",
      "[Epoch 0/200] [Batch 390/938] [D loss: 0.117051] [G loss: 0.637254]\n",
      "[Epoch 0/200] [Batch 391/938] [D loss: 0.098154] [G loss: 0.554685]\n",
      "[Epoch 0/200] [Batch 392/938] [D loss: 0.099808] [G loss: 0.463956]\n",
      "[Epoch 0/200] [Batch 393/938] [D loss: 0.102724] [G loss: 0.472430]\n",
      "[Epoch 0/200] [Batch 394/938] [D loss: 0.085093] [G loss: 0.570731]\n",
      "[Epoch 0/200] [Batch 395/938] [D loss: 0.087905] [G loss: 0.558576]\n",
      "[Epoch 0/200] [Batch 396/938] [D loss: 0.095553] [G loss: 0.539483]\n",
      "[Epoch 0/200] [Batch 397/938] [D loss: 0.093669] [G loss: 0.567634]\n",
      "[Epoch 0/200] [Batch 398/938] [D loss: 0.092863] [G loss: 0.557839]\n",
      "[Epoch 0/200] [Batch 399/938] [D loss: 0.101747] [G loss: 0.619367]\n",
      "[Epoch 0/200] [Batch 400/938] [D loss: 0.126392] [G loss: 0.500342]\n",
      "[Epoch 0/200] [Batch 401/938] [D loss: 0.120364] [G loss: 0.555290]\n",
      "[Epoch 0/200] [Batch 402/938] [D loss: 0.114884] [G loss: 0.440376]\n",
      "[Epoch 0/200] [Batch 403/938] [D loss: 0.113651] [G loss: 0.818473]\n",
      "[Epoch 0/200] [Batch 404/938] [D loss: 0.222335] [G loss: 0.157116]\n",
      "[Epoch 0/200] [Batch 405/938] [D loss: 0.355197] [G loss: 1.736020]\n",
      "[Epoch 0/200] [Batch 406/938] [D loss: 0.328079] [G loss: 0.073622]\n",
      "[Epoch 0/200] [Batch 407/938] [D loss: 0.129944] [G loss: 0.444088]\n",
      "[Epoch 0/200] [Batch 408/938] [D loss: 0.140497] [G loss: 0.743841]\n",
      "[Epoch 0/200] [Batch 409/938] [D loss: 0.127771] [G loss: 0.593674]\n",
      "[Epoch 0/200] [Batch 410/938] [D loss: 0.127459] [G loss: 0.356784]\n",
      "[Epoch 0/200] [Batch 411/938] [D loss: 0.131036] [G loss: 0.409921]\n",
      "[Epoch 0/200] [Batch 412/938] [D loss: 0.125297] [G loss: 0.624710]\n",
      "[Epoch 0/200] [Batch 413/938] [D loss: 0.131344] [G loss: 0.479905]\n",
      "[Epoch 0/200] [Batch 414/938] [D loss: 0.130078] [G loss: 0.396348]\n",
      "[Epoch 0/200] [Batch 415/938] [D loss: 0.120534] [G loss: 0.546532]\n",
      "[Epoch 0/200] [Batch 416/938] [D loss: 0.123385] [G loss: 0.503972]\n",
      "[Epoch 0/200] [Batch 417/938] [D loss: 0.141798] [G loss: 0.446637]\n",
      "[Epoch 0/200] [Batch 418/938] [D loss: 0.126640] [G loss: 0.524187]\n",
      "[Epoch 0/200] [Batch 419/938] [D loss: 0.137330] [G loss: 0.541715]\n",
      "[Epoch 0/200] [Batch 420/938] [D loss: 0.123946] [G loss: 0.387167]\n",
      "[Epoch 0/200] [Batch 421/938] [D loss: 0.140089] [G loss: 0.600091]\n",
      "[Epoch 0/200] [Batch 422/938] [D loss: 0.149602] [G loss: 0.286939]\n",
      "[Epoch 0/200] [Batch 423/938] [D loss: 0.127319] [G loss: 0.910116]\n",
      "[Epoch 0/200] [Batch 424/938] [D loss: 0.142481] [G loss: 0.330034]\n",
      "[Epoch 0/200] [Batch 425/938] [D loss: 0.143687] [G loss: 0.645349]\n",
      "[Epoch 0/200] [Batch 426/938] [D loss: 0.141657] [G loss: 0.378851]\n",
      "[Epoch 0/200] [Batch 427/938] [D loss: 0.137009] [G loss: 0.624997]\n",
      "[Epoch 0/200] [Batch 428/938] [D loss: 0.158463] [G loss: 0.287971]\n",
      "[Epoch 0/200] [Batch 429/938] [D loss: 0.175255] [G loss: 0.907813]\n",
      "[Epoch 0/200] [Batch 430/938] [D loss: 0.278881] [G loss: 0.105445]\n",
      "[Epoch 0/200] [Batch 431/938] [D loss: 0.208771] [G loss: 0.973128]\n",
      "[Epoch 0/200] [Batch 432/938] [D loss: 0.149535] [G loss: 0.323712]\n",
      "[Epoch 0/200] [Batch 433/938] [D loss: 0.141111] [G loss: 0.329401]\n",
      "[Epoch 0/200] [Batch 434/938] [D loss: 0.119372] [G loss: 0.591095]\n",
      "[Epoch 0/200] [Batch 435/938] [D loss: 0.128030] [G loss: 0.583139]\n",
      "[Epoch 0/200] [Batch 436/938] [D loss: 0.152945] [G loss: 0.348180]\n",
      "[Epoch 0/200] [Batch 437/938] [D loss: 0.118715] [G loss: 0.668468]\n",
      "[Epoch 0/200] [Batch 438/938] [D loss: 0.144373] [G loss: 0.358210]\n",
      "[Epoch 0/200] [Batch 439/938] [D loss: 0.133647] [G loss: 0.569575]\n",
      "[Epoch 0/200] [Batch 440/938] [D loss: 0.152260] [G loss: 0.428906]\n",
      "[Epoch 0/200] [Batch 441/938] [D loss: 0.121668] [G loss: 0.531780]\n",
      "[Epoch 0/200] [Batch 442/938] [D loss: 0.124612] [G loss: 0.591690]\n",
      "[Epoch 0/200] [Batch 443/938] [D loss: 0.141209] [G loss: 0.311781]\n",
      "[Epoch 0/200] [Batch 444/938] [D loss: 0.188678] [G loss: 1.046660]\n",
      "[Epoch 0/200] [Batch 445/938] [D loss: 0.545769] [G loss: 0.025480]\n",
      "[Epoch 0/200] [Batch 446/938] [D loss: 0.231408] [G loss: 1.081130]\n",
      "[Epoch 0/200] [Batch 447/938] [D loss: 0.186506] [G loss: 0.819079]\n",
      "[Epoch 0/200] [Batch 448/938] [D loss: 0.149753] [G loss: 0.311099]\n",
      "[Epoch 0/200] [Batch 449/938] [D loss: 0.166147] [G loss: 0.281849]\n",
      "[Epoch 0/200] [Batch 450/938] [D loss: 0.130472] [G loss: 0.471432]\n",
      "[Epoch 0/200] [Batch 451/938] [D loss: 0.130637] [G loss: 0.570750]\n",
      "[Epoch 0/200] [Batch 452/938] [D loss: 0.120436] [G loss: 0.528450]\n",
      "[Epoch 0/200] [Batch 453/938] [D loss: 0.125205] [G loss: 0.449898]\n",
      "[Epoch 0/200] [Batch 454/938] [D loss: 0.118525] [G loss: 0.530219]\n",
      "[Epoch 0/200] [Batch 455/938] [D loss: 0.124605] [G loss: 0.481616]\n",
      "[Epoch 0/200] [Batch 456/938] [D loss: 0.136199] [G loss: 0.479055]\n",
      "[Epoch 0/200] [Batch 457/938] [D loss: 0.135915] [G loss: 0.533141]\n",
      "[Epoch 0/200] [Batch 458/938] [D loss: 0.135704] [G loss: 0.461645]\n",
      "[Epoch 0/200] [Batch 459/938] [D loss: 0.150846] [G loss: 0.519539]\n",
      "[Epoch 0/200] [Batch 460/938] [D loss: 0.163494] [G loss: 0.335659]\n",
      "[Epoch 0/200] [Batch 461/938] [D loss: 0.161851] [G loss: 0.649459]\n",
      "[Epoch 0/200] [Batch 462/938] [D loss: 0.187786] [G loss: 0.193297]\n",
      "[Epoch 0/200] [Batch 463/938] [D loss: 0.271197] [G loss: 1.039733]\n",
      "[Epoch 0/200] [Batch 464/938] [D loss: 0.472211] [G loss: 0.066004]\n",
      "[Epoch 0/200] [Batch 465/938] [D loss: 0.161126] [G loss: 0.813697]\n",
      "[Epoch 0/200] [Batch 466/938] [D loss: 0.194847] [G loss: 0.929764]\n",
      "[Epoch 0/200] [Batch 467/938] [D loss: 0.146209] [G loss: 0.390223]\n",
      "[Epoch 0/200] [Batch 468/938] [D loss: 0.158989] [G loss: 0.266296]\n",
      "[Epoch 0/200] [Batch 469/938] [D loss: 0.119612] [G loss: 0.459114]\n",
      "[Epoch 0/200] [Batch 470/938] [D loss: 0.127978] [G loss: 0.613526]\n",
      "[Epoch 0/200] [Batch 471/938] [D loss: 0.143718] [G loss: 0.414947]\n",
      "[Epoch 0/200] [Batch 472/938] [D loss: 0.126125] [G loss: 0.437408]\n",
      "[Epoch 0/200] [Batch 473/938] [D loss: 0.127151] [G loss: 0.488175]\n",
      "[Epoch 0/200] [Batch 474/938] [D loss: 0.136010] [G loss: 0.501992]\n",
      "[Epoch 0/200] [Batch 475/938] [D loss: 0.146301] [G loss: 0.398206]\n",
      "[Epoch 0/200] [Batch 476/938] [D loss: 0.165040] [G loss: 0.429979]\n",
      "[Epoch 0/200] [Batch 477/938] [D loss: 0.136939] [G loss: 0.390986]\n",
      "[Epoch 0/200] [Batch 478/938] [D loss: 0.129156] [G loss: 0.511290]\n",
      "[Epoch 0/200] [Batch 479/938] [D loss: 0.120107] [G loss: 0.565443]\n",
      "[Epoch 0/200] [Batch 480/938] [D loss: 0.124265] [G loss: 0.458340]\n",
      "[Epoch 0/200] [Batch 481/938] [D loss: 0.127708] [G loss: 0.483584]\n",
      "[Epoch 0/200] [Batch 482/938] [D loss: 0.091930] [G loss: 0.608286]\n",
      "[Epoch 0/200] [Batch 483/938] [D loss: 0.099458] [G loss: 0.595373]\n",
      "[Epoch 0/200] [Batch 484/938] [D loss: 0.103951] [G loss: 0.550165]\n",
      "[Epoch 0/200] [Batch 485/938] [D loss: 0.100609] [G loss: 0.502747]\n",
      "[Epoch 0/200] [Batch 486/938] [D loss: 0.149401] [G loss: 0.874683]\n",
      "[Epoch 0/200] [Batch 487/938] [D loss: 0.443683] [G loss: 0.044084]\n",
      "[Epoch 0/200] [Batch 488/938] [D loss: 0.802086] [G loss: 2.219501]\n",
      "[Epoch 0/200] [Batch 489/938] [D loss: 0.335718] [G loss: 0.057685]\n",
      "[Epoch 0/200] [Batch 490/938] [D loss: 0.234683] [G loss: 0.154990]\n",
      "[Epoch 0/200] [Batch 491/938] [D loss: 0.169445] [G loss: 0.489705]\n",
      "[Epoch 0/200] [Batch 492/938] [D loss: 0.175814] [G loss: 0.634463]\n",
      "[Epoch 0/200] [Batch 493/938] [D loss: 0.151054] [G loss: 0.531823]\n",
      "[Epoch 0/200] [Batch 494/938] [D loss: 0.127950] [G loss: 0.398031]\n",
      "[Epoch 0/200] [Batch 495/938] [D loss: 0.137810] [G loss: 0.364397]\n",
      "[Epoch 0/200] [Batch 496/938] [D loss: 0.104108] [G loss: 0.438150]\n",
      "[Epoch 0/200] [Batch 497/938] [D loss: 0.104160] [G loss: 0.547111]\n",
      "[Epoch 0/200] [Batch 498/938] [D loss: 0.111544] [G loss: 0.572050]\n",
      "[Epoch 0/200] [Batch 499/938] [D loss: 0.117232] [G loss: 0.462550]\n",
      "[Epoch 0/200] [Batch 500/938] [D loss: 0.106032] [G loss: 0.423465]\n",
      "[Epoch 0/200] [Batch 501/938] [D loss: 0.102618] [G loss: 0.538402]\n",
      "[Epoch 0/200] [Batch 502/938] [D loss: 0.104754] [G loss: 0.594087]\n",
      "[Epoch 0/200] [Batch 503/938] [D loss: 0.112293] [G loss: 0.424816]\n",
      "[Epoch 0/200] [Batch 504/938] [D loss: 0.104998] [G loss: 0.515024]\n",
      "[Epoch 0/200] [Batch 505/938] [D loss: 0.120757] [G loss: 0.587255]\n",
      "[Epoch 0/200] [Batch 506/938] [D loss: 0.111610] [G loss: 0.419821]\n",
      "[Epoch 0/200] [Batch 507/938] [D loss: 0.111228] [G loss: 0.634945]\n",
      "[Epoch 0/200] [Batch 508/938] [D loss: 0.107812] [G loss: 0.548942]\n",
      "[Epoch 0/200] [Batch 509/938] [D loss: 0.101910] [G loss: 0.414038]\n",
      "[Epoch 0/200] [Batch 510/938] [D loss: 0.093907] [G loss: 0.708667]\n",
      "[Epoch 0/200] [Batch 511/938] [D loss: 0.107113] [G loss: 0.399724]\n",
      "[Epoch 0/200] [Batch 512/938] [D loss: 0.117867] [G loss: 0.775016]\n",
      "[Epoch 0/200] [Batch 513/938] [D loss: 0.131479] [G loss: 0.360560]\n",
      "[Epoch 0/200] [Batch 514/938] [D loss: 0.118008] [G loss: 0.696728]\n",
      "[Epoch 0/200] [Batch 515/938] [D loss: 0.135452] [G loss: 0.407978]\n",
      "[Epoch 0/200] [Batch 516/938] [D loss: 0.113973] [G loss: 0.576559]\n",
      "[Epoch 0/200] [Batch 517/938] [D loss: 0.109966] [G loss: 0.512290]\n",
      "[Epoch 0/200] [Batch 518/938] [D loss: 0.114991] [G loss: 0.498769]\n",
      "[Epoch 0/200] [Batch 519/938] [D loss: 0.113589] [G loss: 0.536403]\n",
      "[Epoch 0/200] [Batch 520/938] [D loss: 0.132886] [G loss: 0.460735]\n",
      "[Epoch 0/200] [Batch 521/938] [D loss: 0.111353] [G loss: 0.534271]\n",
      "[Epoch 0/200] [Batch 522/938] [D loss: 0.121415] [G loss: 0.545063]\n",
      "[Epoch 0/200] [Batch 523/938] [D loss: 0.129391] [G loss: 0.533449]\n",
      "[Epoch 0/200] [Batch 524/938] [D loss: 0.108639] [G loss: 0.595582]\n",
      "[Epoch 0/200] [Batch 525/938] [D loss: 0.132892] [G loss: 0.439918]\n",
      "[Epoch 0/200] [Batch 526/938] [D loss: 0.131731] [G loss: 0.852111]\n",
      "[Epoch 0/200] [Batch 527/938] [D loss: 0.227829] [G loss: 0.213989]\n",
      "[Epoch 0/200] [Batch 528/938] [D loss: 0.383516] [G loss: 1.540020]\n",
      "[Epoch 0/200] [Batch 529/938] [D loss: 0.325105] [G loss: 0.072216]\n",
      "[Epoch 0/200] [Batch 530/938] [D loss: 0.120817] [G loss: 0.520887]\n",
      "[Epoch 0/200] [Batch 531/938] [D loss: 0.148114] [G loss: 0.811404]\n",
      "[Epoch 0/200] [Batch 532/938] [D loss: 0.110292] [G loss: 0.558038]\n",
      "[Epoch 0/200] [Batch 533/938] [D loss: 0.114187] [G loss: 0.391363]\n",
      "[Epoch 0/200] [Batch 534/938] [D loss: 0.091939] [G loss: 0.571811]\n",
      "[Epoch 0/200] [Batch 535/938] [D loss: 0.085597] [G loss: 0.589643]\n",
      "[Epoch 0/200] [Batch 536/938] [D loss: 0.075646] [G loss: 0.553091]\n",
      "[Epoch 0/200] [Batch 537/938] [D loss: 0.075759] [G loss: 0.566456]\n",
      "[Epoch 0/200] [Batch 538/938] [D loss: 0.083549] [G loss: 0.628121]\n",
      "[Epoch 0/200] [Batch 539/938] [D loss: 0.088354] [G loss: 0.499809]\n",
      "[Epoch 0/200] [Batch 540/938] [D loss: 0.101728] [G loss: 0.658027]\n",
      "[Epoch 0/200] [Batch 541/938] [D loss: 0.108572] [G loss: 0.440232]\n",
      "[Epoch 0/200] [Batch 542/938] [D loss: 0.106674] [G loss: 0.628657]\n",
      "[Epoch 0/200] [Batch 543/938] [D loss: 0.117159] [G loss: 0.458000]\n",
      "[Epoch 0/200] [Batch 544/938] [D loss: 0.111288] [G loss: 0.609939]\n",
      "[Epoch 0/200] [Batch 545/938] [D loss: 0.124014] [G loss: 0.465718]\n",
      "[Epoch 0/200] [Batch 546/938] [D loss: 0.102395] [G loss: 0.561276]\n",
      "[Epoch 0/200] [Batch 547/938] [D loss: 0.117610] [G loss: 0.467951]\n",
      "[Epoch 0/200] [Batch 548/938] [D loss: 0.118311] [G loss: 0.634936]\n",
      "[Epoch 0/200] [Batch 549/938] [D loss: 0.110497] [G loss: 0.533076]\n",
      "[Epoch 0/200] [Batch 550/938] [D loss: 0.094521] [G loss: 0.478044]\n",
      "[Epoch 0/200] [Batch 551/938] [D loss: 0.119604] [G loss: 0.875715]\n",
      "[Epoch 0/200] [Batch 552/938] [D loss: 0.321141] [G loss: 0.082363]\n",
      "[Epoch 0/200] [Batch 553/938] [D loss: 0.661474] [G loss: 2.000798]\n",
      "[Epoch 0/200] [Batch 554/938] [D loss: 0.269384] [G loss: 0.108562]\n",
      "[Epoch 0/200] [Batch 555/938] [D loss: 0.193773] [G loss: 0.227660]\n",
      "[Epoch 0/200] [Batch 556/938] [D loss: 0.155176] [G loss: 0.539102]\n",
      "[Epoch 0/200] [Batch 557/938] [D loss: 0.172409] [G loss: 0.583002]\n",
      "[Epoch 0/200] [Batch 558/938] [D loss: 0.114048] [G loss: 0.441120]\n",
      "[Epoch 0/200] [Batch 559/938] [D loss: 0.131153] [G loss: 0.418121]\n",
      "[Epoch 0/200] [Batch 560/938] [D loss: 0.111581] [G loss: 0.481979]\n",
      "[Epoch 0/200] [Batch 561/938] [D loss: 0.103026] [G loss: 0.551982]\n",
      "[Epoch 0/200] [Batch 562/938] [D loss: 0.091389] [G loss: 0.594287]\n",
      "[Epoch 0/200] [Batch 563/938] [D loss: 0.091427] [G loss: 0.567013]\n",
      "[Epoch 0/200] [Batch 564/938] [D loss: 0.099923] [G loss: 0.514624]\n",
      "[Epoch 0/200] [Batch 565/938] [D loss: 0.086766] [G loss: 0.545214]\n",
      "[Epoch 0/200] [Batch 566/938] [D loss: 0.073130] [G loss: 0.575084]\n",
      "[Epoch 0/200] [Batch 567/938] [D loss: 0.078911] [G loss: 0.640942]\n",
      "[Epoch 0/200] [Batch 568/938] [D loss: 0.074775] [G loss: 0.591313]\n",
      "[Epoch 0/200] [Batch 569/938] [D loss: 0.090175] [G loss: 0.600767]\n",
      "[Epoch 0/200] [Batch 570/938] [D loss: 0.079713] [G loss: 0.652201]\n",
      "[Epoch 0/200] [Batch 571/938] [D loss: 0.102513] [G loss: 0.529317]\n",
      "[Epoch 0/200] [Batch 572/938] [D loss: 0.115194] [G loss: 0.611607]\n",
      "[Epoch 0/200] [Batch 573/938] [D loss: 0.116585] [G loss: 0.428944]\n",
      "[Epoch 0/200] [Batch 574/938] [D loss: 0.117628] [G loss: 0.767966]\n",
      "[Epoch 0/200] [Batch 575/938] [D loss: 0.177897] [G loss: 0.224011]\n",
      "[Epoch 0/200] [Batch 576/938] [D loss: 0.212793] [G loss: 1.064627]\n",
      "[Epoch 0/200] [Batch 577/938] [D loss: 0.218734] [G loss: 0.162582]\n",
      "[Epoch 0/200] [Batch 578/938] [D loss: 0.131083] [G loss: 0.668107]\n",
      "[Epoch 0/200] [Batch 579/938] [D loss: 0.118202] [G loss: 0.626469]\n",
      "[Epoch 0/200] [Batch 580/938] [D loss: 0.123305] [G loss: 0.406021]\n",
      "[Epoch 0/200] [Batch 581/938] [D loss: 0.110909] [G loss: 0.497722]\n",
      "[Epoch 0/200] [Batch 582/938] [D loss: 0.098626] [G loss: 0.647314]\n",
      "[Epoch 0/200] [Batch 583/938] [D loss: 0.107659] [G loss: 0.501036]\n",
      "[Epoch 0/200] [Batch 584/938] [D loss: 0.114413] [G loss: 0.439598]\n",
      "[Epoch 0/200] [Batch 585/938] [D loss: 0.106402] [G loss: 0.601569]\n",
      "[Epoch 0/200] [Batch 586/938] [D loss: 0.103619] [G loss: 0.464719]\n",
      "[Epoch 0/200] [Batch 587/938] [D loss: 0.132475] [G loss: 0.698929]\n",
      "[Epoch 0/200] [Batch 588/938] [D loss: 0.158068] [G loss: 0.273060]\n",
      "[Epoch 0/200] [Batch 589/938] [D loss: 0.135558] [G loss: 0.826866]\n",
      "[Epoch 0/200] [Batch 590/938] [D loss: 0.136092] [G loss: 0.368862]\n",
      "[Epoch 0/200] [Batch 591/938] [D loss: 0.100984] [G loss: 0.679650]\n",
      "[Epoch 0/200] [Batch 592/938] [D loss: 0.097137] [G loss: 0.545590]\n",
      "[Epoch 0/200] [Batch 593/938] [D loss: 0.106638] [G loss: 0.480449]\n",
      "[Epoch 0/200] [Batch 594/938] [D loss: 0.114857] [G loss: 0.833088]\n",
      "[Epoch 0/200] [Batch 595/938] [D loss: 0.132669] [G loss: 0.344105]\n",
      "[Epoch 0/200] [Batch 596/938] [D loss: 0.123360] [G loss: 0.878130]\n",
      "[Epoch 0/200] [Batch 597/938] [D loss: 0.103638] [G loss: 0.460858]\n",
      "[Epoch 0/200] [Batch 598/938] [D loss: 0.088403] [G loss: 0.657173]\n",
      "[Epoch 0/200] [Batch 599/938] [D loss: 0.088431] [G loss: 0.571073]\n",
      "[Epoch 0/200] [Batch 600/938] [D loss: 0.099294] [G loss: 0.672482]\n",
      "[Epoch 0/200] [Batch 601/938] [D loss: 0.086576] [G loss: 0.476571]\n",
      "[Epoch 0/200] [Batch 602/938] [D loss: 0.122662] [G loss: 0.745843]\n",
      "[Epoch 0/200] [Batch 603/938] [D loss: 0.189295] [G loss: 0.230927]\n",
      "[Epoch 0/200] [Batch 604/938] [D loss: 0.331201] [G loss: 1.635541]\n",
      "[Epoch 0/200] [Batch 605/938] [D loss: 0.430619] [G loss: 0.064930]\n",
      "[Epoch 0/200] [Batch 606/938] [D loss: 0.137883] [G loss: 0.775667]\n",
      "[Epoch 0/200] [Batch 607/938] [D loss: 0.193591] [G loss: 0.865174]\n",
      "[Epoch 0/200] [Batch 608/938] [D loss: 0.125050] [G loss: 0.404581]\n",
      "[Epoch 0/200] [Batch 609/938] [D loss: 0.134663] [G loss: 0.352204]\n",
      "[Epoch 0/200] [Batch 610/938] [D loss: 0.121826] [G loss: 0.450458]\n",
      "[Epoch 0/200] [Batch 611/938] [D loss: 0.111062] [G loss: 0.588563]\n",
      "[Epoch 0/200] [Batch 612/938] [D loss: 0.106433] [G loss: 0.501931]\n",
      "[Epoch 0/200] [Batch 613/938] [D loss: 0.094761] [G loss: 0.524289]\n",
      "[Epoch 0/200] [Batch 614/938] [D loss: 0.084921] [G loss: 0.575614]\n",
      "[Epoch 0/200] [Batch 615/938] [D loss: 0.086945] [G loss: 0.606798]\n",
      "[Epoch 0/200] [Batch 616/938] [D loss: 0.109773] [G loss: 0.457515]\n",
      "[Epoch 0/200] [Batch 617/938] [D loss: 0.114421] [G loss: 0.624364]\n",
      "[Epoch 0/200] [Batch 618/938] [D loss: 0.122635] [G loss: 0.402942]\n",
      "[Epoch 0/200] [Batch 619/938] [D loss: 0.123870] [G loss: 0.660328]\n",
      "[Epoch 0/200] [Batch 620/938] [D loss: 0.104598] [G loss: 0.419741]\n",
      "[Epoch 0/200] [Batch 621/938] [D loss: 0.102054] [G loss: 0.661778]\n",
      "[Epoch 0/200] [Batch 622/938] [D loss: 0.116974] [G loss: 0.445478]\n",
      "[Epoch 0/200] [Batch 623/938] [D loss: 0.084999] [G loss: 0.559216]\n",
      "[Epoch 0/200] [Batch 624/938] [D loss: 0.084698] [G loss: 0.597594]\n",
      "[Epoch 0/200] [Batch 625/938] [D loss: 0.097601] [G loss: 0.475859]\n",
      "[Epoch 0/200] [Batch 626/938] [D loss: 0.105947] [G loss: 0.837512]\n",
      "[Epoch 0/200] [Batch 627/938] [D loss: 0.165833] [G loss: 0.258695]\n",
      "[Epoch 0/200] [Batch 628/938] [D loss: 0.187688] [G loss: 1.220134]\n",
      "[Epoch 0/200] [Batch 629/938] [D loss: 0.181777] [G loss: 0.213240]\n",
      "[Epoch 0/200] [Batch 630/938] [D loss: 0.091994] [G loss: 0.673141]\n",
      "[Epoch 0/200] [Batch 631/938] [D loss: 0.121001] [G loss: 0.709551]\n",
      "[Epoch 0/200] [Batch 632/938] [D loss: 0.099979] [G loss: 0.497090]\n",
      "[Epoch 0/200] [Batch 633/938] [D loss: 0.085233] [G loss: 0.530380]\n",
      "[Epoch 0/200] [Batch 634/938] [D loss: 0.099500] [G loss: 0.678732]\n",
      "[Epoch 0/200] [Batch 635/938] [D loss: 0.122421] [G loss: 0.485378]\n",
      "[Epoch 0/200] [Batch 636/938] [D loss: 0.103347] [G loss: 0.439899]\n",
      "[Epoch 0/200] [Batch 637/938] [D loss: 0.120053] [G loss: 0.823680]\n",
      "[Epoch 0/200] [Batch 638/938] [D loss: 0.185755] [G loss: 0.208818]\n",
      "[Epoch 0/200] [Batch 639/938] [D loss: 0.254779] [G loss: 1.408988]\n",
      "[Epoch 0/200] [Batch 640/938] [D loss: 0.182324] [G loss: 0.220590]\n",
      "[Epoch 0/200] [Batch 641/938] [D loss: 0.109073] [G loss: 0.473045]\n",
      "[Epoch 0/200] [Batch 642/938] [D loss: 0.099032] [G loss: 0.687186]\n",
      "[Epoch 0/200] [Batch 643/938] [D loss: 0.092737] [G loss: 0.648999]\n",
      "[Epoch 0/200] [Batch 644/938] [D loss: 0.086632] [G loss: 0.460496]\n",
      "[Epoch 0/200] [Batch 645/938] [D loss: 0.083430] [G loss: 0.608840]\n",
      "[Epoch 0/200] [Batch 646/938] [D loss: 0.075799] [G loss: 0.631389]\n",
      "[Epoch 0/200] [Batch 647/938] [D loss: 0.086753] [G loss: 0.576057]\n",
      "[Epoch 0/200] [Batch 648/938] [D loss: 0.095538] [G loss: 0.494589]\n",
      "[Epoch 0/200] [Batch 649/938] [D loss: 0.080399] [G loss: 0.666487]\n",
      "[Epoch 0/200] [Batch 650/938] [D loss: 0.106669] [G loss: 0.513414]\n",
      "[Epoch 0/200] [Batch 651/938] [D loss: 0.113380] [G loss: 0.514542]\n",
      "[Epoch 0/200] [Batch 652/938] [D loss: 0.112524] [G loss: 0.526109]\n",
      "[Epoch 0/200] [Batch 653/938] [D loss: 0.114289] [G loss: 0.508130]\n",
      "[Epoch 0/200] [Batch 654/938] [D loss: 0.097536] [G loss: 0.588165]\n",
      "[Epoch 0/200] [Batch 655/938] [D loss: 0.094689] [G loss: 0.493133]\n",
      "[Epoch 0/200] [Batch 656/938] [D loss: 0.101709] [G loss: 0.773618]\n",
      "[Epoch 0/200] [Batch 657/938] [D loss: 0.180823] [G loss: 0.206613]\n",
      "[Epoch 0/200] [Batch 658/938] [D loss: 0.264209] [G loss: 1.361844]\n",
      "[Epoch 0/200] [Batch 659/938] [D loss: 0.419150] [G loss: 0.044430]\n",
      "[Epoch 0/200] [Batch 660/938] [D loss: 0.137380] [G loss: 0.677571]\n",
      "[Epoch 0/200] [Batch 661/938] [D loss: 0.150937] [G loss: 0.807123]\n",
      "[Epoch 0/200] [Batch 662/938] [D loss: 0.115266] [G loss: 0.494315]\n",
      "[Epoch 0/200] [Batch 663/938] [D loss: 0.127235] [G loss: 0.363637]\n",
      "[Epoch 0/200] [Batch 664/938] [D loss: 0.109946] [G loss: 0.437736]\n",
      "[Epoch 0/200] [Batch 665/938] [D loss: 0.105824] [G loss: 0.574955]\n",
      "[Epoch 0/200] [Batch 666/938] [D loss: 0.094097] [G loss: 0.582234]\n",
      "[Epoch 0/200] [Batch 667/938] [D loss: 0.085164] [G loss: 0.547981]\n",
      "[Epoch 0/200] [Batch 668/938] [D loss: 0.079525] [G loss: 0.554786]\n",
      "[Epoch 0/200] [Batch 669/938] [D loss: 0.086623] [G loss: 0.555713]\n",
      "[Epoch 0/200] [Batch 670/938] [D loss: 0.082604] [G loss: 0.581078]\n",
      "[Epoch 0/200] [Batch 671/938] [D loss: 0.087896] [G loss: 0.571578]\n",
      "[Epoch 0/200] [Batch 672/938] [D loss: 0.090302] [G loss: 0.651500]\n",
      "[Epoch 0/200] [Batch 673/938] [D loss: 0.074985] [G loss: 0.534478]\n",
      "[Epoch 0/200] [Batch 674/938] [D loss: 0.101000] [G loss: 0.740919]\n",
      "[Epoch 0/200] [Batch 675/938] [D loss: 0.103756] [G loss: 0.436554]\n",
      "[Epoch 0/200] [Batch 676/938] [D loss: 0.094957] [G loss: 0.573701]\n",
      "[Epoch 0/200] [Batch 677/938] [D loss: 0.103133] [G loss: 0.696301]\n",
      "[Epoch 0/200] [Batch 678/938] [D loss: 0.159808] [G loss: 0.293729]\n",
      "[Epoch 0/200] [Batch 679/938] [D loss: 0.270765] [G loss: 1.278381]\n",
      "[Epoch 0/200] [Batch 680/938] [D loss: 0.437670] [G loss: 0.040845]\n",
      "[Epoch 0/200] [Batch 681/938] [D loss: 0.150243] [G loss: 0.763795]\n",
      "[Epoch 0/200] [Batch 682/938] [D loss: 0.161848] [G loss: 0.847072]\n",
      "[Epoch 0/200] [Batch 683/938] [D loss: 0.118307] [G loss: 0.474817]\n",
      "[Epoch 0/200] [Batch 684/938] [D loss: 0.130666] [G loss: 0.362396]\n",
      "[Epoch 0/200] [Batch 685/938] [D loss: 0.114665] [G loss: 0.497762]\n",
      "[Epoch 0/200] [Batch 686/938] [D loss: 0.113403] [G loss: 0.576252]\n",
      "[Epoch 0/200] [Batch 687/938] [D loss: 0.104123] [G loss: 0.572265]\n",
      "[Epoch 0/200] [Batch 688/938] [D loss: 0.103279] [G loss: 0.510538]\n",
      "[Epoch 0/200] [Batch 689/938] [D loss: 0.081730] [G loss: 0.561009]\n",
      "[Epoch 0/200] [Batch 690/938] [D loss: 0.090867] [G loss: 0.617243]\n",
      "[Epoch 0/200] [Batch 691/938] [D loss: 0.094441] [G loss: 0.548661]\n",
      "[Epoch 0/200] [Batch 692/938] [D loss: 0.103290] [G loss: 0.585414]\n",
      "[Epoch 0/200] [Batch 693/938] [D loss: 0.125041] [G loss: 0.454681]\n",
      "[Epoch 0/200] [Batch 694/938] [D loss: 0.114395] [G loss: 0.586518]\n",
      "[Epoch 0/200] [Batch 695/938] [D loss: 0.107662] [G loss: 0.485760]\n",
      "[Epoch 0/200] [Batch 696/938] [D loss: 0.097279] [G loss: 0.651623]\n",
      "[Epoch 0/200] [Batch 697/938] [D loss: 0.116846] [G loss: 0.518035]\n",
      "[Epoch 0/200] [Batch 698/938] [D loss: 0.116328] [G loss: 0.477520]\n",
      "[Epoch 0/200] [Batch 699/938] [D loss: 0.116119] [G loss: 0.693984]\n",
      "[Epoch 0/200] [Batch 700/938] [D loss: 0.158407] [G loss: 0.331329]\n",
      "[Epoch 0/200] [Batch 701/938] [D loss: 0.223120] [G loss: 1.189629]\n",
      "[Epoch 0/200] [Batch 702/938] [D loss: 0.487689] [G loss: 0.022350]\n",
      "[Epoch 0/200] [Batch 703/938] [D loss: 0.222170] [G loss: 0.997231]\n",
      "[Epoch 0/200] [Batch 704/938] [D loss: 0.166105] [G loss: 0.584778]\n",
      "[Epoch 0/200] [Batch 705/938] [D loss: 0.141985] [G loss: 0.342931]\n",
      "[Epoch 0/200] [Batch 706/938] [D loss: 0.125229] [G loss: 0.414757]\n",
      "[Epoch 0/200] [Batch 707/938] [D loss: 0.113930] [G loss: 0.501451]\n",
      "[Epoch 0/200] [Batch 708/938] [D loss: 0.106527] [G loss: 0.607021]\n",
      "[Epoch 0/200] [Batch 709/938] [D loss: 0.101441] [G loss: 0.546840]\n",
      "[Epoch 0/200] [Batch 710/938] [D loss: 0.093025] [G loss: 0.528826]\n",
      "[Epoch 0/200] [Batch 711/938] [D loss: 0.084611] [G loss: 0.570274]\n",
      "[Epoch 0/200] [Batch 712/938] [D loss: 0.081755] [G loss: 0.652410]\n",
      "[Epoch 0/200] [Batch 713/938] [D loss: 0.094328] [G loss: 0.592171]\n",
      "[Epoch 0/200] [Batch 714/938] [D loss: 0.090561] [G loss: 0.574443]\n",
      "[Epoch 0/200] [Batch 715/938] [D loss: 0.104514] [G loss: 0.532380]\n",
      "[Epoch 0/200] [Batch 716/938] [D loss: 0.098992] [G loss: 0.485702]\n",
      "[Epoch 0/200] [Batch 717/938] [D loss: 0.112638] [G loss: 0.722894]\n",
      "[Epoch 0/200] [Batch 718/938] [D loss: 0.112415] [G loss: 0.421285]\n",
      "[Epoch 0/200] [Batch 719/938] [D loss: 0.111644] [G loss: 0.695296]\n",
      "[Epoch 0/200] [Batch 720/938] [D loss: 0.101012] [G loss: 0.458321]\n",
      "[Epoch 0/200] [Batch 721/938] [D loss: 0.113517] [G loss: 0.900084]\n",
      "[Epoch 0/200] [Batch 722/938] [D loss: 0.178391] [G loss: 0.228655]\n",
      "[Epoch 0/200] [Batch 723/938] [D loss: 0.164132] [G loss: 0.929638]\n",
      "[Epoch 0/200] [Batch 724/938] [D loss: 0.138158] [G loss: 0.347078]\n",
      "[Epoch 0/200] [Batch 725/938] [D loss: 0.100882] [G loss: 0.643705]\n",
      "[Epoch 0/200] [Batch 726/938] [D loss: 0.124920] [G loss: 0.626129]\n",
      "[Epoch 0/200] [Batch 727/938] [D loss: 0.115277] [G loss: 0.414780]\n",
      "[Epoch 0/200] [Batch 728/938] [D loss: 0.102459] [G loss: 0.585889]\n",
      "[Epoch 0/200] [Batch 729/938] [D loss: 0.114516] [G loss: 0.514481]\n",
      "[Epoch 0/200] [Batch 730/938] [D loss: 0.112034] [G loss: 0.487562]\n",
      "[Epoch 0/200] [Batch 731/938] [D loss: 0.109850] [G loss: 0.428333]\n",
      "[Epoch 0/200] [Batch 732/938] [D loss: 0.114542] [G loss: 0.750783]\n",
      "[Epoch 0/200] [Batch 733/938] [D loss: 0.130862] [G loss: 0.351179]\n",
      "[Epoch 0/200] [Batch 734/938] [D loss: 0.105223] [G loss: 0.703955]\n",
      "[Epoch 0/200] [Batch 735/938] [D loss: 0.107777] [G loss: 0.422090]\n",
      "[Epoch 0/200] [Batch 736/938] [D loss: 0.114281] [G loss: 0.672008]\n",
      "[Epoch 0/200] [Batch 737/938] [D loss: 0.129340] [G loss: 0.412489]\n",
      "[Epoch 0/200] [Batch 738/938] [D loss: 0.123248] [G loss: 0.706730]\n",
      "[Epoch 0/200] [Batch 739/938] [D loss: 0.110422] [G loss: 0.382540]\n",
      "[Epoch 0/200] [Batch 740/938] [D loss: 0.133015] [G loss: 0.953034]\n",
      "[Epoch 0/200] [Batch 741/938] [D loss: 0.162822] [G loss: 0.277314]\n",
      "[Epoch 0/200] [Batch 742/938] [D loss: 0.132098] [G loss: 0.908096]\n",
      "[Epoch 0/200] [Batch 743/938] [D loss: 0.103974] [G loss: 0.408077]\n",
      "[Epoch 0/200] [Batch 744/938] [D loss: 0.105147] [G loss: 0.660420]\n",
      "[Epoch 0/200] [Batch 745/938] [D loss: 0.090920] [G loss: 0.553087]\n",
      "[Epoch 0/200] [Batch 746/938] [D loss: 0.076526] [G loss: 0.708017]\n",
      "[Epoch 0/200] [Batch 747/938] [D loss: 0.069959] [G loss: 0.689895]\n",
      "[Epoch 0/200] [Batch 748/938] [D loss: 0.067444] [G loss: 0.618279]\n",
      "[Epoch 0/200] [Batch 749/938] [D loss: 0.082734] [G loss: 0.761669]\n",
      "[Epoch 0/200] [Batch 750/938] [D loss: 0.094915] [G loss: 0.497059]\n",
      "[Epoch 0/200] [Batch 751/938] [D loss: 0.157381] [G loss: 1.004208]\n",
      "[Epoch 0/200] [Batch 752/938] [D loss: 0.321193] [G loss: 0.087078]\n",
      "[Epoch 0/200] [Batch 753/938] [D loss: 0.302775] [G loss: 1.575076]\n",
      "[Epoch 0/200] [Batch 754/938] [D loss: 0.126459] [G loss: 0.386141]\n",
      "[Epoch 0/200] [Batch 755/938] [D loss: 0.149707] [G loss: 0.287122]\n",
      "[Epoch 0/200] [Batch 756/938] [D loss: 0.109775] [G loss: 0.588846]\n",
      "[Epoch 0/200] [Batch 757/938] [D loss: 0.099034] [G loss: 0.656000]\n",
      "[Epoch 0/200] [Batch 758/938] [D loss: 0.085632] [G loss: 0.526108]\n",
      "[Epoch 0/200] [Batch 759/938] [D loss: 0.092558] [G loss: 0.576770]\n",
      "[Epoch 0/200] [Batch 760/938] [D loss: 0.070094] [G loss: 0.610482]\n",
      "[Epoch 0/200] [Batch 761/938] [D loss: 0.091215] [G loss: 0.633747]\n",
      "[Epoch 0/200] [Batch 762/938] [D loss: 0.089494] [G loss: 0.579691]\n",
      "[Epoch 0/200] [Batch 763/938] [D loss: 0.083515] [G loss: 0.593857]\n",
      "[Epoch 0/200] [Batch 764/938] [D loss: 0.107583] [G loss: 0.739872]\n",
      "[Epoch 0/200] [Batch 765/938] [D loss: 0.122896] [G loss: 0.427085]\n",
      "[Epoch 0/200] [Batch 766/938] [D loss: 0.109587] [G loss: 0.764622]\n",
      "[Epoch 0/200] [Batch 767/938] [D loss: 0.148211] [G loss: 0.324101]\n",
      "[Epoch 0/200] [Batch 768/938] [D loss: 0.152093] [G loss: 0.903679]\n",
      "[Epoch 0/200] [Batch 769/938] [D loss: 0.245899] [G loss: 0.164641]\n",
      "[Epoch 0/200] [Batch 770/938] [D loss: 0.280067] [G loss: 1.244044]\n",
      "[Epoch 0/200] [Batch 771/938] [D loss: 0.274797] [G loss: 0.123482]\n",
      "[Epoch 0/200] [Batch 772/938] [D loss: 0.124198] [G loss: 0.643791]\n",
      "[Epoch 0/200] [Batch 773/938] [D loss: 0.105943] [G loss: 0.769699]\n",
      "[Epoch 0/200] [Batch 774/938] [D loss: 0.110208] [G loss: 0.543669]\n",
      "[Epoch 0/200] [Batch 775/938] [D loss: 0.106092] [G loss: 0.430412]\n",
      "[Epoch 0/200] [Batch 776/938] [D loss: 0.091971] [G loss: 0.674095]\n",
      "[Epoch 0/200] [Batch 777/938] [D loss: 0.099709] [G loss: 0.643248]\n",
      "[Epoch 0/200] [Batch 778/938] [D loss: 0.098535] [G loss: 0.587770]\n",
      "[Epoch 0/200] [Batch 779/938] [D loss: 0.088728] [G loss: 0.643823]\n",
      "[Epoch 0/200] [Batch 780/938] [D loss: 0.093009] [G loss: 0.558438]\n",
      "[Epoch 0/200] [Batch 781/938] [D loss: 0.098575] [G loss: 0.579403]\n",
      "[Epoch 0/200] [Batch 782/938] [D loss: 0.097750] [G loss: 0.747374]\n",
      "[Epoch 0/200] [Batch 783/938] [D loss: 0.154464] [G loss: 0.274653]\n",
      "[Epoch 0/200] [Batch 784/938] [D loss: 0.167232] [G loss: 1.058391]\n",
      "[Epoch 0/200] [Batch 785/938] [D loss: 0.247701] [G loss: 0.147206]\n",
      "[Epoch 0/200] [Batch 786/938] [D loss: 0.219047] [G loss: 1.226269]\n",
      "[Epoch 0/200] [Batch 787/938] [D loss: 0.158086] [G loss: 0.285962]\n",
      "[Epoch 0/200] [Batch 788/938] [D loss: 0.138009] [G loss: 0.421393]\n",
      "[Epoch 0/200] [Batch 789/938] [D loss: 0.119840] [G loss: 0.639535]\n",
      "[Epoch 0/200] [Batch 790/938] [D loss: 0.114186] [G loss: 0.576564]\n",
      "[Epoch 0/200] [Batch 791/938] [D loss: 0.092453] [G loss: 0.543590]\n",
      "[Epoch 0/200] [Batch 792/938] [D loss: 0.080860] [G loss: 0.645435]\n",
      "[Epoch 0/200] [Batch 793/938] [D loss: 0.108725] [G loss: 0.548405]\n",
      "[Epoch 0/200] [Batch 794/938] [D loss: 0.094086] [G loss: 0.482258]\n",
      "[Epoch 0/200] [Batch 795/938] [D loss: 0.079964] [G loss: 0.758323]\n",
      "[Epoch 0/200] [Batch 796/938] [D loss: 0.105455] [G loss: 0.435165]\n",
      "[Epoch 0/200] [Batch 797/938] [D loss: 0.113180] [G loss: 0.814089]\n",
      "[Epoch 0/200] [Batch 798/938] [D loss: 0.129777] [G loss: 0.347505]\n",
      "[Epoch 0/200] [Batch 799/938] [D loss: 0.153386] [G loss: 0.898204]\n",
      "[Epoch 0/200] [Batch 800/938] [D loss: 0.248924] [G loss: 0.147172]\n",
      "[Epoch 0/200] [Batch 801/938] [D loss: 0.316805] [G loss: 1.591172]\n",
      "[Epoch 0/200] [Batch 802/938] [D loss: 0.206998] [G loss: 0.183775]\n",
      "[Epoch 0/200] [Batch 803/938] [D loss: 0.110917] [G loss: 0.444633]\n",
      "[Epoch 0/200] [Batch 804/938] [D loss: 0.124007] [G loss: 0.796553]\n",
      "[Epoch 0/200] [Batch 805/938] [D loss: 0.104962] [G loss: 0.676993]\n",
      "[Epoch 0/200] [Batch 806/938] [D loss: 0.095724] [G loss: 0.418792]\n",
      "[Epoch 0/200] [Batch 807/938] [D loss: 0.099075] [G loss: 0.477439]\n",
      "[Epoch 0/200] [Batch 808/938] [D loss: 0.082310] [G loss: 0.697586]\n",
      "[Epoch 0/200] [Batch 809/938] [D loss: 0.083676] [G loss: 0.618486]\n",
      "[Epoch 0/200] [Batch 810/938] [D loss: 0.078720] [G loss: 0.527962]\n",
      "[Epoch 0/200] [Batch 811/938] [D loss: 0.074879] [G loss: 0.697829]\n",
      "[Epoch 0/200] [Batch 812/938] [D loss: 0.096031] [G loss: 0.511914]\n",
      "[Epoch 0/200] [Batch 813/938] [D loss: 0.097798] [G loss: 0.770929]\n",
      "[Epoch 0/200] [Batch 814/938] [D loss: 0.091641] [G loss: 0.411552]\n",
      "[Epoch 0/200] [Batch 815/938] [D loss: 0.107569] [G loss: 0.939642]\n",
      "[Epoch 0/200] [Batch 816/938] [D loss: 0.140724] [G loss: 0.319672]\n",
      "[Epoch 0/200] [Batch 817/938] [D loss: 0.108079] [G loss: 0.808396]\n",
      "[Epoch 0/200] [Batch 818/938] [D loss: 0.115956] [G loss: 0.448756]\n",
      "[Epoch 0/200] [Batch 819/938] [D loss: 0.122240] [G loss: 0.798354]\n",
      "[Epoch 0/200] [Batch 820/938] [D loss: 0.141511] [G loss: 0.333066]\n",
      "[Epoch 0/200] [Batch 821/938] [D loss: 0.127557] [G loss: 0.965867]\n",
      "[Epoch 0/200] [Batch 822/938] [D loss: 0.140917] [G loss: 0.314292]\n",
      "[Epoch 0/200] [Batch 823/938] [D loss: 0.145348] [G loss: 1.058810]\n",
      "[Epoch 0/200] [Batch 824/938] [D loss: 0.124595] [G loss: 0.325263]\n",
      "[Epoch 0/200] [Batch 825/938] [D loss: 0.079458] [G loss: 0.722634]\n",
      "[Epoch 0/200] [Batch 826/938] [D loss: 0.081654] [G loss: 0.639913]\n",
      "[Epoch 0/200] [Batch 827/938] [D loss: 0.089041] [G loss: 0.433091]\n",
      "[Epoch 0/200] [Batch 828/938] [D loss: 0.079599] [G loss: 0.769078]\n",
      "[Epoch 0/200] [Batch 829/938] [D loss: 0.087978] [G loss: 0.447983]\n",
      "[Epoch 0/200] [Batch 830/938] [D loss: 0.089619] [G loss: 0.788421]\n",
      "[Epoch 0/200] [Batch 831/938] [D loss: 0.097119] [G loss: 0.444688]\n",
      "[Epoch 0/200] [Batch 832/938] [D loss: 0.111150] [G loss: 0.883648]\n",
      "[Epoch 0/200] [Batch 833/938] [D loss: 0.189400] [G loss: 0.242134]\n",
      "[Epoch 0/200] [Batch 834/938] [D loss: 0.273505] [G loss: 1.406847]\n",
      "[Epoch 0/200] [Batch 835/938] [D loss: 0.280556] [G loss: 0.097724]\n",
      "[Epoch 0/200] [Batch 836/938] [D loss: 0.100065] [G loss: 0.657542]\n",
      "[Epoch 0/200] [Batch 837/938] [D loss: 0.134201] [G loss: 0.862330]\n",
      "[Epoch 0/200] [Batch 838/938] [D loss: 0.104887] [G loss: 0.458872]\n",
      "[Epoch 0/200] [Batch 839/938] [D loss: 0.115890] [G loss: 0.428501]\n",
      "[Epoch 0/200] [Batch 840/938] [D loss: 0.074294] [G loss: 0.692503]\n",
      "[Epoch 0/200] [Batch 841/938] [D loss: 0.077608] [G loss: 0.712666]\n",
      "[Epoch 0/200] [Batch 842/938] [D loss: 0.085792] [G loss: 0.572814]\n",
      "[Epoch 0/200] [Batch 843/938] [D loss: 0.080448] [G loss: 0.734847]\n",
      "[Epoch 0/200] [Batch 844/938] [D loss: 0.075333] [G loss: 0.613606]\n",
      "[Epoch 0/200] [Batch 845/938] [D loss: 0.077174] [G loss: 0.525327]\n",
      "[Epoch 0/200] [Batch 846/938] [D loss: 0.089695] [G loss: 0.761017]\n",
      "[Epoch 0/200] [Batch 847/938] [D loss: 0.082105] [G loss: 0.622745]\n",
      "[Epoch 0/200] [Batch 848/938] [D loss: 0.086896] [G loss: 0.500238]\n",
      "[Epoch 0/200] [Batch 849/938] [D loss: 0.082881] [G loss: 0.704983]\n",
      "[Epoch 0/200] [Batch 850/938] [D loss: 0.096766] [G loss: 0.645303]\n",
      "[Epoch 0/200] [Batch 851/938] [D loss: 0.096350] [G loss: 0.530101]\n",
      "[Epoch 0/200] [Batch 852/938] [D loss: 0.093845] [G loss: 0.644831]\n",
      "[Epoch 0/200] [Batch 853/938] [D loss: 0.097416] [G loss: 0.526927]\n",
      "[Epoch 0/200] [Batch 854/938] [D loss: 0.110547] [G loss: 0.684143]\n",
      "[Epoch 0/200] [Batch 855/938] [D loss: 0.123237] [G loss: 0.407400]\n",
      "[Epoch 0/200] [Batch 856/938] [D loss: 0.174619] [G loss: 1.141853]\n",
      "[Epoch 0/200] [Batch 857/938] [D loss: 0.388399] [G loss: 0.063088]\n",
      "[Epoch 0/200] [Batch 858/938] [D loss: 0.478189] [G loss: 1.894144]\n",
      "[Epoch 0/200] [Batch 859/938] [D loss: 0.122092] [G loss: 0.397507]\n",
      "[Epoch 0/200] [Batch 860/938] [D loss: 0.156171] [G loss: 0.217180]\n",
      "[Epoch 0/200] [Batch 861/938] [D loss: 0.105462] [G loss: 0.480159]\n",
      "[Epoch 0/200] [Batch 862/938] [D loss: 0.110904] [G loss: 0.688706]\n",
      "[Epoch 0/200] [Batch 863/938] [D loss: 0.096948] [G loss: 0.649847]\n",
      "[Epoch 0/200] [Batch 864/938] [D loss: 0.073262] [G loss: 0.565988]\n",
      "[Epoch 0/200] [Batch 865/938] [D loss: 0.077206] [G loss: 0.536387]\n",
      "[Epoch 0/200] [Batch 866/938] [D loss: 0.072652] [G loss: 0.689211]\n",
      "[Epoch 0/200] [Batch 867/938] [D loss: 0.063930] [G loss: 0.715305]\n",
      "[Epoch 0/200] [Batch 868/938] [D loss: 0.059961] [G loss: 0.681139]\n",
      "[Epoch 0/200] [Batch 869/938] [D loss: 0.060895] [G loss: 0.613562]\n",
      "[Epoch 0/200] [Batch 870/938] [D loss: 0.070665] [G loss: 0.674894]\n",
      "[Epoch 0/200] [Batch 871/938] [D loss: 0.058842] [G loss: 0.661725]\n",
      "[Epoch 0/200] [Batch 872/938] [D loss: 0.071306] [G loss: 0.707699]\n",
      "[Epoch 0/200] [Batch 873/938] [D loss: 0.083460] [G loss: 0.627875]\n",
      "[Epoch 0/200] [Batch 874/938] [D loss: 0.079816] [G loss: 0.556779]\n",
      "[Epoch 0/200] [Batch 875/938] [D loss: 0.091752] [G loss: 0.688780]\n",
      "[Epoch 0/200] [Batch 876/938] [D loss: 0.070274] [G loss: 0.591347]\n",
      "[Epoch 0/200] [Batch 877/938] [D loss: 0.084497] [G loss: 0.742936]\n",
      "[Epoch 0/200] [Batch 878/938] [D loss: 0.100128] [G loss: 0.508007]\n",
      "[Epoch 0/200] [Batch 879/938] [D loss: 0.109325] [G loss: 0.845353]\n",
      "[Epoch 0/200] [Batch 880/938] [D loss: 0.152196] [G loss: 0.317181]\n",
      "[Epoch 0/200] [Batch 881/938] [D loss: 0.163292] [G loss: 1.109126]\n",
      "[Epoch 0/200] [Batch 882/938] [D loss: 0.152765] [G loss: 0.277358]\n",
      "[Epoch 0/200] [Batch 883/938] [D loss: 0.098861] [G loss: 0.814377]\n",
      "[Epoch 0/200] [Batch 884/938] [D loss: 0.098667] [G loss: 0.653172]\n",
      "[Epoch 0/200] [Batch 885/938] [D loss: 0.091833] [G loss: 0.480371]\n",
      "[Epoch 0/200] [Batch 886/938] [D loss: 0.077658] [G loss: 0.679252]\n",
      "[Epoch 0/200] [Batch 887/938] [D loss: 0.090734] [G loss: 0.735577]\n",
      "[Epoch 0/200] [Batch 888/938] [D loss: 0.101560] [G loss: 0.458582]\n",
      "[Epoch 0/200] [Batch 889/938] [D loss: 0.082660] [G loss: 0.786837]\n",
      "[Epoch 0/200] [Batch 890/938] [D loss: 0.091463] [G loss: 0.485006]\n",
      "[Epoch 0/200] [Batch 891/938] [D loss: 0.149831] [G loss: 1.041659]\n",
      "[Epoch 0/200] [Batch 892/938] [D loss: 0.177990] [G loss: 0.221126]\n",
      "[Epoch 0/200] [Batch 893/938] [D loss: 0.159927] [G loss: 1.038313]\n",
      "[Epoch 0/200] [Batch 894/938] [D loss: 0.110019] [G loss: 0.448126]\n",
      "[Epoch 0/200] [Batch 895/938] [D loss: 0.076650] [G loss: 0.660650]\n",
      "[Epoch 0/200] [Batch 896/938] [D loss: 0.088411] [G loss: 0.733350]\n",
      "[Epoch 0/200] [Batch 897/938] [D loss: 0.105189] [G loss: 0.558614]\n",
      "[Epoch 0/200] [Batch 898/938] [D loss: 0.094335] [G loss: 0.507199]\n",
      "[Epoch 0/200] [Batch 899/938] [D loss: 0.106196] [G loss: 0.851558]\n",
      "[Epoch 0/200] [Batch 900/938] [D loss: 0.077567] [G loss: 0.486584]\n",
      "[Epoch 0/200] [Batch 901/938] [D loss: 0.068143] [G loss: 0.851609]\n",
      "[Epoch 0/200] [Batch 902/938] [D loss: 0.096189] [G loss: 0.516154]\n",
      "[Epoch 0/200] [Batch 903/938] [D loss: 0.104156] [G loss: 0.906918]\n",
      "[Epoch 0/200] [Batch 904/938] [D loss: 0.157276] [G loss: 0.291388]\n",
      "[Epoch 0/200] [Batch 905/938] [D loss: 0.209806] [G loss: 1.398096]\n",
      "[Epoch 0/200] [Batch 906/938] [D loss: 0.275050] [G loss: 0.148416]\n",
      "[Epoch 0/200] [Batch 907/938] [D loss: 0.166111] [G loss: 1.037493]\n",
      "[Epoch 0/200] [Batch 908/938] [D loss: 0.093589] [G loss: 0.652210]\n",
      "[Epoch 0/200] [Batch 909/938] [D loss: 0.103827] [G loss: 0.440058]\n",
      "[Epoch 0/200] [Batch 910/938] [D loss: 0.082334] [G loss: 0.662391]\n",
      "[Epoch 0/200] [Batch 911/938] [D loss: 0.080904] [G loss: 0.695770]\n",
      "[Epoch 0/200] [Batch 912/938] [D loss: 0.077028] [G loss: 0.630116]\n",
      "[Epoch 0/200] [Batch 913/938] [D loss: 0.083308] [G loss: 0.577274]\n",
      "[Epoch 0/200] [Batch 914/938] [D loss: 0.072856] [G loss: 0.612501]\n",
      "[Epoch 0/200] [Batch 915/938] [D loss: 0.085434] [G loss: 0.649866]\n",
      "[Epoch 0/200] [Batch 916/938] [D loss: 0.070613] [G loss: 0.663633]\n",
      "[Epoch 0/200] [Batch 917/938] [D loss: 0.083280] [G loss: 0.607089]\n",
      "[Epoch 0/200] [Batch 918/938] [D loss: 0.079159] [G loss: 0.631592]\n",
      "[Epoch 0/200] [Batch 919/938] [D loss: 0.104523] [G loss: 0.740211]\n",
      "[Epoch 0/200] [Batch 920/938] [D loss: 0.089735] [G loss: 0.462670]\n",
      "[Epoch 0/200] [Batch 921/938] [D loss: 0.099457] [G loss: 0.845635]\n",
      "[Epoch 0/200] [Batch 922/938] [D loss: 0.131146] [G loss: 0.351515]\n",
      "[Epoch 0/200] [Batch 923/938] [D loss: 0.236981] [G loss: 1.338054]\n",
      "[Epoch 0/200] [Batch 924/938] [D loss: 0.341251] [G loss: 0.088724]\n",
      "[Epoch 0/200] [Batch 925/938] [D loss: 0.166424] [G loss: 1.008881]\n",
      "[Epoch 0/200] [Batch 926/938] [D loss: 0.094474] [G loss: 0.684467]\n",
      "[Epoch 0/200] [Batch 927/938] [D loss: 0.113525] [G loss: 0.456736]\n",
      "[Epoch 0/200] [Batch 928/938] [D loss: 0.097408] [G loss: 0.546623]\n",
      "[Epoch 0/200] [Batch 929/938] [D loss: 0.095556] [G loss: 0.665810]\n",
      "[Epoch 0/200] [Batch 930/938] [D loss: 0.087487] [G loss: 0.567029]\n",
      "[Epoch 0/200] [Batch 931/938] [D loss: 0.082059] [G loss: 0.623836]\n",
      "[Epoch 0/200] [Batch 932/938] [D loss: 0.075713] [G loss: 0.663791]\n",
      "[Epoch 0/200] [Batch 933/938] [D loss: 0.082426] [G loss: 0.597888]\n",
      "[Epoch 0/200] [Batch 934/938] [D loss: 0.076399] [G loss: 0.725271]\n",
      "[Epoch 0/200] [Batch 935/938] [D loss: 0.079566] [G loss: 0.621687]\n",
      "[Epoch 0/200] [Batch 936/938] [D loss: 0.068677] [G loss: 0.793735]\n",
      "[Epoch 0/200] [Batch 937/938] [D loss: 0.087370] [G loss: 0.596196]\n",
      "[Epoch 1/200] [Batch 0/938] [D loss: 0.086706] [G loss: 0.645989]\n",
      "[Epoch 1/200] [Batch 1/938] [D loss: 0.101856] [G loss: 0.714392]\n",
      "[Epoch 1/200] [Batch 2/938] [D loss: 0.113532] [G loss: 0.401093]\n",
      "[Epoch 1/200] [Batch 3/938] [D loss: 0.140712] [G loss: 1.111296]\n",
      "[Epoch 1/200] [Batch 4/938] [D loss: 0.168696] [G loss: 0.258661]\n",
      "[Epoch 1/200] [Batch 5/938] [D loss: 0.110195] [G loss: 0.877134]\n",
      "[Epoch 1/200] [Batch 6/938] [D loss: 0.095104] [G loss: 0.574887]\n",
      "[Epoch 1/200] [Batch 7/938] [D loss: 0.091371] [G loss: 0.616191]\n",
      "[Epoch 1/200] [Batch 8/938] [D loss: 0.087615] [G loss: 0.676116]\n",
      "[Epoch 1/200] [Batch 9/938] [D loss: 0.091452] [G loss: 0.567851]\n",
      "[Epoch 1/200] [Batch 10/938] [D loss: 0.087362] [G loss: 0.661016]\n",
      "[Epoch 1/200] [Batch 11/938] [D loss: 0.086522] [G loss: 0.638044]\n",
      "[Epoch 1/200] [Batch 12/938] [D loss: 0.099297] [G loss: 0.560124]\n",
      "[Epoch 1/200] [Batch 13/938] [D loss: 0.089605] [G loss: 0.610554]\n",
      "[Epoch 1/200] [Batch 14/938] [D loss: 0.094662] [G loss: 0.688500]\n",
      "[Epoch 1/200] [Batch 15/938] [D loss: 0.098289] [G loss: 0.630538]\n",
      "[Epoch 1/200] [Batch 16/938] [D loss: 0.105593] [G loss: 0.535457]\n",
      "[Epoch 1/200] [Batch 17/938] [D loss: 0.144462] [G loss: 0.919400]\n",
      "[Epoch 1/200] [Batch 18/938] [D loss: 0.362476] [G loss: 0.095958]\n",
      "[Epoch 1/200] [Batch 19/938] [D loss: 0.743604] [G loss: 2.404039]\n",
      "[Epoch 1/200] [Batch 20/938] [D loss: 0.213134] [G loss: 0.175921]\n",
      "[Epoch 1/200] [Batch 21/938] [D loss: 0.175200] [G loss: 0.245532]\n",
      "[Epoch 1/200] [Batch 22/938] [D loss: 0.127950] [G loss: 0.568995]\n",
      "[Epoch 1/200] [Batch 23/938] [D loss: 0.136446] [G loss: 0.671894]\n",
      "[Epoch 1/200] [Batch 24/938] [D loss: 0.109975] [G loss: 0.589486]\n",
      "[Epoch 1/200] [Batch 25/938] [D loss: 0.113730] [G loss: 0.480295]\n",
      "[Epoch 1/200] [Batch 26/938] [D loss: 0.103046] [G loss: 0.510995]\n",
      "[Epoch 1/200] [Batch 27/938] [D loss: 0.086134] [G loss: 0.568087]\n",
      "[Epoch 1/200] [Batch 28/938] [D loss: 0.078133] [G loss: 0.518399]\n",
      "[Epoch 1/200] [Batch 29/938] [D loss: 0.088674] [G loss: 0.673298]\n",
      "[Epoch 1/200] [Batch 30/938] [D loss: 0.085628] [G loss: 0.599581]\n",
      "[Epoch 1/200] [Batch 31/938] [D loss: 0.089610] [G loss: 0.601932]\n",
      "[Epoch 1/200] [Batch 32/938] [D loss: 0.079005] [G loss: 0.537983]\n",
      "[Epoch 1/200] [Batch 33/938] [D loss: 0.086414] [G loss: 0.630077]\n",
      "[Epoch 1/200] [Batch 34/938] [D loss: 0.085144] [G loss: 0.634124]\n",
      "[Epoch 1/200] [Batch 35/938] [D loss: 0.078387] [G loss: 0.611125]\n",
      "[Epoch 1/200] [Batch 36/938] [D loss: 0.107325] [G loss: 0.574239]\n",
      "[Epoch 1/200] [Batch 37/938] [D loss: 0.088659] [G loss: 0.544460]\n",
      "[Epoch 1/200] [Batch 38/938] [D loss: 0.094784] [G loss: 0.637479]\n",
      "[Epoch 1/200] [Batch 39/938] [D loss: 0.107142] [G loss: 0.574601]\n",
      "[Epoch 1/200] [Batch 40/938] [D loss: 0.103448] [G loss: 0.548439]\n",
      "[Epoch 1/200] [Batch 41/938] [D loss: 0.122767] [G loss: 0.535310]\n",
      "[Epoch 1/200] [Batch 42/938] [D loss: 0.141038] [G loss: 0.434138]\n",
      "[Epoch 1/200] [Batch 43/938] [D loss: 0.105922] [G loss: 0.544033]\n",
      "[Epoch 1/200] [Batch 44/938] [D loss: 0.125560] [G loss: 0.639471]\n",
      "[Epoch 1/200] [Batch 45/938] [D loss: 0.124059] [G loss: 0.450793]\n",
      "[Epoch 1/200] [Batch 46/938] [D loss: 0.148527] [G loss: 0.734461]\n",
      "[Epoch 1/200] [Batch 47/938] [D loss: 0.158516] [G loss: 0.284537]\n",
      "[Epoch 1/200] [Batch 48/938] [D loss: 0.143346] [G loss: 0.941600]\n",
      "[Epoch 1/200] [Batch 49/938] [D loss: 0.114532] [G loss: 0.472999]\n",
      "[Epoch 1/200] [Batch 50/938] [D loss: 0.115905] [G loss: 0.659477]\n",
      "[Epoch 1/200] [Batch 51/938] [D loss: 0.099751] [G loss: 0.565365]\n",
      "[Epoch 1/200] [Batch 52/938] [D loss: 0.091962] [G loss: 0.668183]\n",
      "[Epoch 1/200] [Batch 53/938] [D loss: 0.121726] [G loss: 0.682504]\n",
      "[Epoch 1/200] [Batch 54/938] [D loss: 0.136527] [G loss: 0.381620]\n",
      "[Epoch 1/200] [Batch 55/938] [D loss: 0.097978] [G loss: 0.654540]\n",
      "[Epoch 1/200] [Batch 56/938] [D loss: 0.098968] [G loss: 0.608366]\n",
      "[Epoch 1/200] [Batch 57/938] [D loss: 0.099955] [G loss: 0.534392]\n",
      "[Epoch 1/200] [Batch 58/938] [D loss: 0.102988] [G loss: 0.642686]\n",
      "[Epoch 1/200] [Batch 59/938] [D loss: 0.083325] [G loss: 0.653805]\n",
      "[Epoch 1/200] [Batch 60/938] [D loss: 0.081681] [G loss: 0.586755]\n",
      "[Epoch 1/200] [Batch 61/938] [D loss: 0.072668] [G loss: 0.653349]\n",
      "[Epoch 1/200] [Batch 62/938] [D loss: 0.089690] [G loss: 0.597898]\n",
      "[Epoch 1/200] [Batch 63/938] [D loss: 0.087574] [G loss: 0.706126]\n",
      "[Epoch 1/200] [Batch 64/938] [D loss: 0.094540] [G loss: 0.675824]\n",
      "[Epoch 1/200] [Batch 65/938] [D loss: 0.097613] [G loss: 0.517768]\n",
      "[Epoch 1/200] [Batch 66/938] [D loss: 0.087447] [G loss: 0.644306]\n",
      "[Epoch 1/200] [Batch 67/938] [D loss: 0.091012] [G loss: 0.760953]\n",
      "[Epoch 1/200] [Batch 68/938] [D loss: 0.157234] [G loss: 0.295778]\n",
      "[Epoch 1/200] [Batch 69/938] [D loss: 0.281359] [G loss: 1.545814]\n",
      "[Epoch 1/200] [Batch 70/938] [D loss: 0.481635] [G loss: 0.030777]\n",
      "[Epoch 1/200] [Batch 71/938] [D loss: 0.144062] [G loss: 0.833068]\n",
      "[Epoch 1/200] [Batch 72/938] [D loss: 0.150468] [G loss: 0.915367]\n",
      "[Epoch 1/200] [Batch 73/938] [D loss: 0.112263] [G loss: 0.528278]\n",
      "[Epoch 1/200] [Batch 74/938] [D loss: 0.115765] [G loss: 0.375624]\n",
      "[Epoch 1/200] [Batch 75/938] [D loss: 0.092623] [G loss: 0.545185]\n",
      "[Epoch 1/200] [Batch 76/938] [D loss: 0.067690] [G loss: 0.723195]\n",
      "[Epoch 1/200] [Batch 77/938] [D loss: 0.082647] [G loss: 0.706606]\n",
      "[Epoch 1/200] [Batch 78/938] [D loss: 0.075904] [G loss: 0.627655]\n",
      "[Epoch 1/200] [Batch 79/938] [D loss: 0.071605] [G loss: 0.537571]\n",
      "[Epoch 1/200] [Batch 80/938] [D loss: 0.072286] [G loss: 0.703882]\n",
      "[Epoch 1/200] [Batch 81/938] [D loss: 0.070369] [G loss: 0.634883]\n",
      "[Epoch 1/200] [Batch 82/938] [D loss: 0.083716] [G loss: 0.711581]\n",
      "[Epoch 1/200] [Batch 83/938] [D loss: 0.085004] [G loss: 0.584322]\n",
      "[Epoch 1/200] [Batch 84/938] [D loss: 0.079756] [G loss: 0.637567]\n",
      "[Epoch 1/200] [Batch 85/938] [D loss: 0.091382] [G loss: 0.682341]\n",
      "[Epoch 1/200] [Batch 86/938] [D loss: 0.100142] [G loss: 0.514922]\n",
      "[Epoch 1/200] [Batch 87/938] [D loss: 0.092632] [G loss: 0.542444]\n",
      "[Epoch 1/200] [Batch 88/938] [D loss: 0.101552] [G loss: 0.608468]\n",
      "[Epoch 1/200] [Batch 89/938] [D loss: 0.086246] [G loss: 0.515195]\n",
      "[Epoch 1/200] [Batch 90/938] [D loss: 0.097935] [G loss: 0.644051]\n",
      "[Epoch 1/200] [Batch 91/938] [D loss: 0.111880] [G loss: 0.563383]\n",
      "[Epoch 1/200] [Batch 92/938] [D loss: 0.093980] [G loss: 0.525723]\n",
      "[Epoch 1/200] [Batch 93/938] [D loss: 0.110951] [G loss: 0.694994]\n",
      "[Epoch 1/200] [Batch 94/938] [D loss: 0.125290] [G loss: 0.430797]\n",
      "[Epoch 1/200] [Batch 95/938] [D loss: 0.143176] [G loss: 1.036375]\n",
      "[Epoch 1/200] [Batch 96/938] [D loss: 0.224643] [G loss: 0.181487]\n",
      "[Epoch 1/200] [Batch 97/938] [D loss: 0.172981] [G loss: 1.015204]\n",
      "[Epoch 1/200] [Batch 98/938] [D loss: 0.117975] [G loss: 0.490434]\n",
      "[Epoch 1/200] [Batch 99/938] [D loss: 0.130319] [G loss: 0.410571]\n",
      "[Epoch 1/200] [Batch 100/938] [D loss: 0.103913] [G loss: 0.711599]\n",
      "[Epoch 1/200] [Batch 101/938] [D loss: 0.089913] [G loss: 0.649887]\n",
      "[Epoch 1/200] [Batch 102/938] [D loss: 0.081652] [G loss: 0.483166]\n",
      "[Epoch 1/200] [Batch 103/938] [D loss: 0.095112] [G loss: 0.654791]\n",
      "[Epoch 1/200] [Batch 104/938] [D loss: 0.102632] [G loss: 0.589754]\n",
      "[Epoch 1/200] [Batch 105/938] [D loss: 0.111508] [G loss: 0.539231]\n",
      "[Epoch 1/200] [Batch 106/938] [D loss: 0.093970] [G loss: 0.591666]\n",
      "[Epoch 1/200] [Batch 107/938] [D loss: 0.091833] [G loss: 0.659483]\n",
      "[Epoch 1/200] [Batch 108/938] [D loss: 0.086163] [G loss: 0.611752]\n",
      "[Epoch 1/200] [Batch 109/938] [D loss: 0.091544] [G loss: 0.696276]\n",
      "[Epoch 1/200] [Batch 110/938] [D loss: 0.084928] [G loss: 0.713824]\n",
      "[Epoch 1/200] [Batch 111/938] [D loss: 0.095082] [G loss: 0.538758]\n",
      "[Epoch 1/200] [Batch 112/938] [D loss: 0.112221] [G loss: 0.810923]\n",
      "[Epoch 1/200] [Batch 113/938] [D loss: 0.153984] [G loss: 0.319322]\n",
      "[Epoch 1/200] [Batch 114/938] [D loss: 0.189769] [G loss: 1.192190]\n",
      "[Epoch 1/200] [Batch 115/938] [D loss: 0.152492] [G loss: 0.267083]\n",
      "[Epoch 1/200] [Batch 116/938] [D loss: 0.096290] [G loss: 0.674995]\n",
      "[Epoch 1/200] [Batch 117/938] [D loss: 0.080048] [G loss: 0.650725]\n",
      "[Epoch 1/200] [Batch 118/938] [D loss: 0.088403] [G loss: 0.575661]\n",
      "[Epoch 1/200] [Batch 119/938] [D loss: 0.089276] [G loss: 0.618910]\n",
      "[Epoch 1/200] [Batch 120/938] [D loss: 0.069750] [G loss: 0.653284]\n",
      "[Epoch 1/200] [Batch 121/938] [D loss: 0.083626] [G loss: 0.664544]\n",
      "[Epoch 1/200] [Batch 122/938] [D loss: 0.069633] [G loss: 0.649179]\n",
      "[Epoch 1/200] [Batch 123/938] [D loss: 0.089558] [G loss: 0.758190]\n",
      "[Epoch 1/200] [Batch 124/938] [D loss: 0.079412] [G loss: 0.630435]\n",
      "[Epoch 1/200] [Batch 125/938] [D loss: 0.083354] [G loss: 0.730958]\n",
      "[Epoch 1/200] [Batch 126/938] [D loss: 0.103840] [G loss: 0.511751]\n",
      "[Epoch 1/200] [Batch 127/938] [D loss: 0.090086] [G loss: 0.723446]\n",
      "[Epoch 1/200] [Batch 128/938] [D loss: 0.101983] [G loss: 0.577513]\n",
      "[Epoch 1/200] [Batch 129/938] [D loss: 0.084356] [G loss: 0.629676]\n",
      "[Epoch 1/200] [Batch 130/938] [D loss: 0.097650] [G loss: 0.681972]\n",
      "[Epoch 1/200] [Batch 131/938] [D loss: 0.101811] [G loss: 0.595248]\n",
      "[Epoch 1/200] [Batch 132/938] [D loss: 0.091309] [G loss: 0.623688]\n",
      "[Epoch 1/200] [Batch 133/938] [D loss: 0.085128] [G loss: 0.669942]\n",
      "[Epoch 1/200] [Batch 134/938] [D loss: 0.090675] [G loss: 0.604145]\n",
      "[Epoch 1/200] [Batch 135/938] [D loss: 0.100755] [G loss: 0.793859]\n",
      "[Epoch 1/200] [Batch 136/938] [D loss: 0.159807] [G loss: 0.256469]\n",
      "[Epoch 1/200] [Batch 137/938] [D loss: 0.273697] [G loss: 1.428464]\n",
      "[Epoch 1/200] [Batch 138/938] [D loss: 0.624132] [G loss: 0.019168]\n",
      "[Epoch 1/200] [Batch 139/938] [D loss: 0.209519] [G loss: 1.049343]\n",
      "[Epoch 1/200] [Batch 140/938] [D loss: 0.173437] [G loss: 0.973355]\n",
      "[Epoch 1/200] [Batch 141/938] [D loss: 0.111905] [G loss: 0.490648]\n",
      "[Epoch 1/200] [Batch 142/938] [D loss: 0.130808] [G loss: 0.350209]\n",
      "[Epoch 1/200] [Batch 143/938] [D loss: 0.104019] [G loss: 0.482620]\n",
      "[Epoch 1/200] [Batch 144/938] [D loss: 0.087815] [G loss: 0.631433]\n",
      "[Epoch 1/200] [Batch 145/938] [D loss: 0.082468] [G loss: 0.714464]\n",
      "[Epoch 1/200] [Batch 146/938] [D loss: 0.075992] [G loss: 0.581115]\n",
      "[Epoch 1/200] [Batch 147/938] [D loss: 0.078013] [G loss: 0.625188]\n",
      "[Epoch 1/200] [Batch 148/938] [D loss: 0.067691] [G loss: 0.689406]\n",
      "[Epoch 1/200] [Batch 149/938] [D loss: 0.073151] [G loss: 0.626258]\n",
      "[Epoch 1/200] [Batch 150/938] [D loss: 0.083124] [G loss: 0.703544]\n",
      "[Epoch 1/200] [Batch 151/938] [D loss: 0.075119] [G loss: 0.618819]\n",
      "[Epoch 1/200] [Batch 152/938] [D loss: 0.091662] [G loss: 0.548538]\n",
      "[Epoch 1/200] [Batch 153/938] [D loss: 0.067764] [G loss: 0.778461]\n",
      "[Epoch 1/200] [Batch 154/938] [D loss: 0.083446] [G loss: 0.671293]\n",
      "[Epoch 1/200] [Batch 155/938] [D loss: 0.079088] [G loss: 0.613974]\n",
      "[Epoch 1/200] [Batch 156/938] [D loss: 0.088246] [G loss: 0.642584]\n",
      "[Epoch 1/200] [Batch 157/938] [D loss: 0.072307] [G loss: 0.632777]\n",
      "[Epoch 1/200] [Batch 158/938] [D loss: 0.091283] [G loss: 0.583506]\n",
      "[Epoch 1/200] [Batch 159/938] [D loss: 0.101334] [G loss: 0.484609]\n",
      "[Epoch 1/200] [Batch 160/938] [D loss: 0.109443] [G loss: 0.722455]\n",
      "[Epoch 1/200] [Batch 161/938] [D loss: 0.108777] [G loss: 0.450885]\n",
      "[Epoch 1/200] [Batch 162/938] [D loss: 0.109422] [G loss: 0.780592]\n",
      "[Epoch 1/200] [Batch 163/938] [D loss: 0.126911] [G loss: 0.405217]\n",
      "[Epoch 1/200] [Batch 164/938] [D loss: 0.150430] [G loss: 0.991148]\n",
      "[Epoch 1/200] [Batch 165/938] [D loss: 0.219739] [G loss: 0.205486]\n",
      "[Epoch 1/200] [Batch 166/938] [D loss: 0.211422] [G loss: 1.076773]\n",
      "[Epoch 1/200] [Batch 167/938] [D loss: 0.115785] [G loss: 0.400129]\n",
      "[Epoch 1/200] [Batch 168/938] [D loss: 0.109576] [G loss: 0.436503]\n",
      "[Epoch 1/200] [Batch 169/938] [D loss: 0.132844] [G loss: 0.784873]\n",
      "[Epoch 1/200] [Batch 170/938] [D loss: 0.095763] [G loss: 0.539619]\n",
      "[Epoch 1/200] [Batch 171/938] [D loss: 0.105279] [G loss: 0.500440]\n",
      "[Epoch 1/200] [Batch 172/938] [D loss: 0.091998] [G loss: 0.663547]\n",
      "[Epoch 1/200] [Batch 173/938] [D loss: 0.092220] [G loss: 0.578438]\n",
      "[Epoch 1/200] [Batch 174/938] [D loss: 0.116891] [G loss: 0.547270]\n",
      "[Epoch 1/200] [Batch 175/938] [D loss: 0.101548] [G loss: 0.677633]\n",
      "[Epoch 1/200] [Batch 176/938] [D loss: 0.092034] [G loss: 0.601546]\n",
      "[Epoch 1/200] [Batch 177/938] [D loss: 0.089026] [G loss: 0.575570]\n",
      "[Epoch 1/200] [Batch 178/938] [D loss: 0.092058] [G loss: 0.654946]\n",
      "[Epoch 1/200] [Batch 179/938] [D loss: 0.104932] [G loss: 0.513427]\n",
      "[Epoch 1/200] [Batch 180/938] [D loss: 0.124741] [G loss: 0.654997]\n",
      "[Epoch 1/200] [Batch 181/938] [D loss: 0.092025] [G loss: 0.485956]\n",
      "[Epoch 1/200] [Batch 182/938] [D loss: 0.091120] [G loss: 0.774545]\n",
      "[Epoch 1/200] [Batch 183/938] [D loss: 0.095045] [G loss: 0.749090]\n",
      "[Epoch 1/200] [Batch 184/938] [D loss: 0.138751] [G loss: 0.387136]\n",
      "[Epoch 1/200] [Batch 185/938] [D loss: 0.177344] [G loss: 1.179652]\n",
      "[Epoch 1/200] [Batch 186/938] [D loss: 0.263123] [G loss: 0.134268]\n",
      "[Epoch 1/200] [Batch 187/938] [D loss: 0.129621] [G loss: 1.009245]\n",
      "[Epoch 1/200] [Batch 188/938] [D loss: 0.108971] [G loss: 0.700542]\n",
      "[Epoch 1/200] [Batch 189/938] [D loss: 0.110803] [G loss: 0.431540]\n",
      "[Epoch 1/200] [Batch 190/938] [D loss: 0.095903] [G loss: 0.629100]\n",
      "[Epoch 1/200] [Batch 191/938] [D loss: 0.091544] [G loss: 0.719066]\n",
      "[Epoch 1/200] [Batch 192/938] [D loss: 0.079704] [G loss: 0.646266]\n",
      "[Epoch 1/200] [Batch 193/938] [D loss: 0.088632] [G loss: 0.579668]\n",
      "[Epoch 1/200] [Batch 194/938] [D loss: 0.069286] [G loss: 0.643391]\n",
      "[Epoch 1/200] [Batch 195/938] [D loss: 0.079480] [G loss: 0.581686]\n",
      "[Epoch 1/200] [Batch 196/938] [D loss: 0.067726] [G loss: 0.621807]\n",
      "[Epoch 1/200] [Batch 197/938] [D loss: 0.086957] [G loss: 0.671317]\n",
      "[Epoch 1/200] [Batch 198/938] [D loss: 0.074804] [G loss: 0.581736]\n",
      "[Epoch 1/200] [Batch 199/938] [D loss: 0.125457] [G loss: 1.063969]\n",
      "[Epoch 1/200] [Batch 200/938] [D loss: 0.182628] [G loss: 0.256621]\n",
      "[Epoch 1/200] [Batch 201/938] [D loss: 0.226403] [G loss: 1.384695]\n",
      "[Epoch 1/200] [Batch 202/938] [D loss: 0.182664] [G loss: 0.227202]\n",
      "[Epoch 1/200] [Batch 203/938] [D loss: 0.106575] [G loss: 0.553826]\n",
      "[Epoch 1/200] [Batch 204/938] [D loss: 0.122314] [G loss: 0.760114]\n",
      "[Epoch 1/200] [Batch 205/938] [D loss: 0.112895] [G loss: 0.489504]\n",
      "[Epoch 1/200] [Batch 206/938] [D loss: 0.097958] [G loss: 0.525557]\n",
      "[Epoch 1/200] [Batch 207/938] [D loss: 0.095607] [G loss: 0.704157]\n",
      "[Epoch 1/200] [Batch 208/938] [D loss: 0.082840] [G loss: 0.666680]\n",
      "[Epoch 1/200] [Batch 209/938] [D loss: 0.110630] [G loss: 0.479778]\n",
      "[Epoch 1/200] [Batch 210/938] [D loss: 0.085955] [G loss: 0.791382]\n",
      "[Epoch 1/200] [Batch 211/938] [D loss: 0.068957] [G loss: 0.592258]\n",
      "[Epoch 1/200] [Batch 212/938] [D loss: 0.092655] [G loss: 0.796765]\n",
      "[Epoch 1/200] [Batch 213/938] [D loss: 0.091950] [G loss: 0.525226]\n",
      "[Epoch 1/200] [Batch 214/938] [D loss: 0.096459] [G loss: 0.625864]\n",
      "[Epoch 1/200] [Batch 215/938] [D loss: 0.072320] [G loss: 0.778394]\n",
      "[Epoch 1/200] [Batch 216/938] [D loss: 0.076474] [G loss: 0.577599]\n",
      "[Epoch 1/200] [Batch 217/938] [D loss: 0.111462] [G loss: 0.679914]\n",
      "[Epoch 1/200] [Batch 218/938] [D loss: 0.108598] [G loss: 0.473218]\n",
      "[Epoch 1/200] [Batch 219/938] [D loss: 0.119963] [G loss: 0.879886]\n",
      "[Epoch 1/200] [Batch 220/938] [D loss: 0.114783] [G loss: 0.401024]\n",
      "[Epoch 1/200] [Batch 221/938] [D loss: 0.107766] [G loss: 0.938637]\n",
      "[Epoch 1/200] [Batch 222/938] [D loss: 0.124662] [G loss: 0.379348]\n",
      "[Epoch 1/200] [Batch 223/938] [D loss: 0.134064] [G loss: 0.948260]\n",
      "[Epoch 1/200] [Batch 224/938] [D loss: 0.125669] [G loss: 0.378825]\n",
      "[Epoch 1/200] [Batch 225/938] [D loss: 0.113264] [G loss: 0.913306]\n",
      "[Epoch 1/200] [Batch 226/938] [D loss: 0.090509] [G loss: 0.530114]\n",
      "[Epoch 1/200] [Batch 227/938] [D loss: 0.085940] [G loss: 0.553768]\n",
      "[Epoch 1/200] [Batch 228/938] [D loss: 0.127221] [G loss: 0.972180]\n",
      "[Epoch 1/200] [Batch 229/938] [D loss: 0.151676] [G loss: 0.291208]\n",
      "[Epoch 1/200] [Batch 230/938] [D loss: 0.124645] [G loss: 1.057367]\n",
      "[Epoch 1/200] [Batch 231/938] [D loss: 0.092308] [G loss: 0.440317]\n",
      "[Epoch 1/200] [Batch 232/938] [D loss: 0.087558] [G loss: 0.721778]\n",
      "[Epoch 1/200] [Batch 233/938] [D loss: 0.081629] [G loss: 0.580468]\n",
      "[Epoch 1/200] [Batch 234/938] [D loss: 0.086311] [G loss: 0.701317]\n",
      "[Epoch 1/200] [Batch 235/938] [D loss: 0.074893] [G loss: 0.647193]\n",
      "[Epoch 1/200] [Batch 236/938] [D loss: 0.078270] [G loss: 0.620701]\n",
      "[Epoch 1/200] [Batch 237/938] [D loss: 0.100905] [G loss: 0.576653]\n",
      "[Epoch 1/200] [Batch 238/938] [D loss: 0.100396] [G loss: 0.690715]\n",
      "[Epoch 1/200] [Batch 239/938] [D loss: 0.103705] [G loss: 0.508356]\n",
      "[Epoch 1/200] [Batch 240/938] [D loss: 0.115341] [G loss: 0.584847]\n",
      "[Epoch 1/200] [Batch 241/938] [D loss: 0.088197] [G loss: 0.636272]\n",
      "[Epoch 1/200] [Batch 242/938] [D loss: 0.086374] [G loss: 0.599719]\n",
      "[Epoch 1/200] [Batch 243/938] [D loss: 0.081655] [G loss: 0.818003]\n",
      "[Epoch 1/200] [Batch 244/938] [D loss: 0.105687] [G loss: 0.464243]\n",
      "[Epoch 1/200] [Batch 245/938] [D loss: 0.173835] [G loss: 1.135308]\n",
      "[Epoch 1/200] [Batch 246/938] [D loss: 0.353856] [G loss: 0.063474]\n",
      "[Epoch 1/200] [Batch 247/938] [D loss: 0.375205] [G loss: 1.618427]\n",
      "[Epoch 1/200] [Batch 248/938] [D loss: 0.138660] [G loss: 0.367670]\n",
      "[Epoch 1/200] [Batch 249/938] [D loss: 0.122775] [G loss: 0.326848]\n",
      "[Epoch 1/200] [Batch 250/938] [D loss: 0.108112] [G loss: 0.632870]\n",
      "[Epoch 1/200] [Batch 251/938] [D loss: 0.113890] [G loss: 0.696682]\n",
      "[Epoch 1/200] [Batch 252/938] [D loss: 0.087803] [G loss: 0.553478]\n",
      "[Epoch 1/200] [Batch 253/938] [D loss: 0.072204] [G loss: 0.553321]\n",
      "[Epoch 1/200] [Batch 254/938] [D loss: 0.070696] [G loss: 0.750515]\n",
      "[Epoch 1/200] [Batch 255/938] [D loss: 0.072939] [G loss: 0.693207]\n",
      "[Epoch 1/200] [Batch 256/938] [D loss: 0.100844] [G loss: 0.595899]\n",
      "[Epoch 1/200] [Batch 257/938] [D loss: 0.076764] [G loss: 0.581976]\n",
      "[Epoch 1/200] [Batch 258/938] [D loss: 0.083032] [G loss: 0.704751]\n",
      "[Epoch 1/200] [Batch 259/938] [D loss: 0.087912] [G loss: 0.592503]\n",
      "[Epoch 1/200] [Batch 260/938] [D loss: 0.073517] [G loss: 0.643273]\n",
      "[Epoch 1/200] [Batch 261/938] [D loss: 0.073758] [G loss: 0.676685]\n",
      "[Epoch 1/200] [Batch 262/938] [D loss: 0.079121] [G loss: 0.580426]\n",
      "[Epoch 1/200] [Batch 263/938] [D loss: 0.086232] [G loss: 0.629604]\n",
      "[Epoch 1/200] [Batch 264/938] [D loss: 0.092475] [G loss: 0.682714]\n",
      "[Epoch 1/200] [Batch 265/938] [D loss: 0.109080] [G loss: 0.524150]\n",
      "[Epoch 1/200] [Batch 266/938] [D loss: 0.097995] [G loss: 0.763928]\n",
      "[Epoch 1/200] [Batch 267/938] [D loss: 0.101836] [G loss: 0.485028]\n",
      "[Epoch 1/200] [Batch 268/938] [D loss: 0.075604] [G loss: 0.743577]\n",
      "[Epoch 1/200] [Batch 269/938] [D loss: 0.083306] [G loss: 0.618996]\n",
      "[Epoch 1/200] [Batch 270/938] [D loss: 0.090149] [G loss: 0.606227]\n",
      "[Epoch 1/200] [Batch 271/938] [D loss: 0.099965] [G loss: 0.772462]\n",
      "[Epoch 1/200] [Batch 272/938] [D loss: 0.104997] [G loss: 0.427503]\n",
      "[Epoch 1/200] [Batch 273/938] [D loss: 0.137431] [G loss: 1.231752]\n",
      "[Epoch 1/200] [Batch 274/938] [D loss: 0.182063] [G loss: 0.231226]\n",
      "[Epoch 1/200] [Batch 275/938] [D loss: 0.112951] [G loss: 1.037227]\n",
      "[Epoch 1/200] [Batch 276/938] [D loss: 0.071743] [G loss: 0.571107]\n",
      "[Epoch 1/200] [Batch 277/938] [D loss: 0.071339] [G loss: 0.627869]\n",
      "[Epoch 1/200] [Batch 278/938] [D loss: 0.075597] [G loss: 0.687299]\n",
      "[Epoch 1/200] [Batch 279/938] [D loss: 0.071506] [G loss: 0.638856]\n",
      "[Epoch 1/200] [Batch 280/938] [D loss: 0.076041] [G loss: 0.778952]\n",
      "[Epoch 1/200] [Batch 281/938] [D loss: 0.070999] [G loss: 0.552901]\n",
      "[Epoch 1/200] [Batch 282/938] [D loss: 0.084829] [G loss: 0.617216]\n",
      "[Epoch 1/200] [Batch 283/938] [D loss: 0.079281] [G loss: 0.652110]\n",
      "[Epoch 1/200] [Batch 284/938] [D loss: 0.085111] [G loss: 0.746217]\n",
      "[Epoch 1/200] [Batch 285/938] [D loss: 0.096539] [G loss: 0.506273]\n",
      "[Epoch 1/200] [Batch 286/938] [D loss: 0.131341] [G loss: 1.023592]\n",
      "[Epoch 1/200] [Batch 287/938] [D loss: 0.219832] [G loss: 0.177715]\n",
      "[Epoch 1/200] [Batch 288/938] [D loss: 0.275098] [G loss: 1.404346]\n",
      "[Epoch 1/200] [Batch 289/938] [D loss: 0.180122] [G loss: 0.199447]\n",
      "[Epoch 1/200] [Batch 290/938] [D loss: 0.093934] [G loss: 0.522468]\n",
      "[Epoch 1/200] [Batch 291/938] [D loss: 0.101466] [G loss: 0.858987]\n",
      "[Epoch 1/200] [Batch 292/938] [D loss: 0.091827] [G loss: 0.574635]\n",
      "[Epoch 1/200] [Batch 293/938] [D loss: 0.084744] [G loss: 0.546102]\n",
      "[Epoch 1/200] [Batch 294/938] [D loss: 0.084193] [G loss: 0.714633]\n",
      "[Epoch 1/200] [Batch 295/938] [D loss: 0.080649] [G loss: 0.663847]\n",
      "[Epoch 1/200] [Batch 296/938] [D loss: 0.074729] [G loss: 0.673733]\n",
      "[Epoch 1/200] [Batch 297/938] [D loss: 0.085712] [G loss: 0.516072]\n",
      "[Epoch 1/200] [Batch 298/938] [D loss: 0.088258] [G loss: 0.659405]\n",
      "[Epoch 1/200] [Batch 299/938] [D loss: 0.096769] [G loss: 0.659898]\n",
      "[Epoch 1/200] [Batch 300/938] [D loss: 0.084376] [G loss: 0.535319]\n",
      "[Epoch 1/200] [Batch 301/938] [D loss: 0.086728] [G loss: 0.869008]\n",
      "[Epoch 1/200] [Batch 302/938] [D loss: 0.092937] [G loss: 0.577367]\n",
      "[Epoch 1/200] [Batch 303/938] [D loss: 0.086040] [G loss: 0.692880]\n",
      "[Epoch 1/200] [Batch 304/938] [D loss: 0.103855] [G loss: 0.612098]\n",
      "[Epoch 1/200] [Batch 305/938] [D loss: 0.105379] [G loss: 0.626542]\n",
      "[Epoch 1/200] [Batch 306/938] [D loss: 0.108102] [G loss: 0.513612]\n",
      "[Epoch 1/200] [Batch 307/938] [D loss: 0.129221] [G loss: 0.700827]\n",
      "[Epoch 1/200] [Batch 308/938] [D loss: 0.150222] [G loss: 0.263392]\n",
      "[Epoch 1/200] [Batch 309/938] [D loss: 0.285882] [G loss: 1.309718]\n",
      "[Epoch 1/200] [Batch 310/938] [D loss: 0.539095] [G loss: 0.030272]\n",
      "[Epoch 1/200] [Batch 311/938] [D loss: 0.247925] [G loss: 1.078398]\n",
      "[Epoch 1/200] [Batch 312/938] [D loss: 0.138974] [G loss: 0.689377]\n",
      "[Epoch 1/200] [Batch 313/938] [D loss: 0.156089] [G loss: 0.315714]\n",
      "[Epoch 1/200] [Batch 314/938] [D loss: 0.114622] [G loss: 0.425258]\n",
      "[Epoch 1/200] [Batch 315/938] [D loss: 0.114200] [G loss: 0.609284]\n",
      "[Epoch 1/200] [Batch 316/938] [D loss: 0.097652] [G loss: 0.589940]\n",
      "[Epoch 1/200] [Batch 317/938] [D loss: 0.091542] [G loss: 0.515448]\n",
      "[Epoch 1/200] [Batch 318/938] [D loss: 0.087957] [G loss: 0.650033]\n",
      "[Epoch 1/200] [Batch 319/938] [D loss: 0.084852] [G loss: 0.538001]\n",
      "[Epoch 1/200] [Batch 320/938] [D loss: 0.080212] [G loss: 0.711237]\n",
      "[Epoch 1/200] [Batch 321/938] [D loss: 0.069257] [G loss: 0.535082]\n",
      "[Epoch 1/200] [Batch 322/938] [D loss: 0.070024] [G loss: 0.642421]\n",
      "[Epoch 1/200] [Batch 323/938] [D loss: 0.088631] [G loss: 0.682777]\n",
      "[Epoch 1/200] [Batch 324/938] [D loss: 0.100267] [G loss: 0.547152]\n",
      "[Epoch 1/200] [Batch 325/938] [D loss: 0.078316] [G loss: 0.689261]\n",
      "[Epoch 1/200] [Batch 326/938] [D loss: 0.090296] [G loss: 0.654918]\n",
      "[Epoch 1/200] [Batch 327/938] [D loss: 0.092180] [G loss: 0.627906]\n",
      "[Epoch 1/200] [Batch 328/938] [D loss: 0.104315] [G loss: 0.495878]\n",
      "[Epoch 1/200] [Batch 329/938] [D loss: 0.117632] [G loss: 0.707780]\n",
      "[Epoch 1/200] [Batch 330/938] [D loss: 0.113697] [G loss: 0.426073]\n",
      "[Epoch 1/200] [Batch 331/938] [D loss: 0.100617] [G loss: 0.762149]\n",
      "[Epoch 1/200] [Batch 332/938] [D loss: 0.109899] [G loss: 0.400570]\n",
      "[Epoch 1/200] [Batch 333/938] [D loss: 0.119297] [G loss: 0.814395]\n",
      "[Epoch 1/200] [Batch 334/938] [D loss: 0.153039] [G loss: 0.274545]\n",
      "[Epoch 1/200] [Batch 335/938] [D loss: 0.159390] [G loss: 0.987040]\n",
      "[Epoch 1/200] [Batch 336/938] [D loss: 0.173184] [G loss: 0.250030]\n",
      "[Epoch 1/200] [Batch 337/938] [D loss: 0.108409] [G loss: 0.821636]\n",
      "[Epoch 1/200] [Batch 338/938] [D loss: 0.115782] [G loss: 0.500097]\n",
      "[Epoch 1/200] [Batch 339/938] [D loss: 0.125285] [G loss: 0.482480]\n",
      "[Epoch 1/200] [Batch 340/938] [D loss: 0.097576] [G loss: 0.727918]\n",
      "[Epoch 1/200] [Batch 341/938] [D loss: 0.098375] [G loss: 0.532527]\n",
      "[Epoch 1/200] [Batch 342/938] [D loss: 0.101731] [G loss: 0.630836]\n",
      "[Epoch 1/200] [Batch 343/938] [D loss: 0.098099] [G loss: 0.440687]\n",
      "[Epoch 1/200] [Batch 344/938] [D loss: 0.118565] [G loss: 0.931301]\n",
      "[Epoch 1/200] [Batch 345/938] [D loss: 0.158528] [G loss: 0.302493]\n",
      "[Epoch 1/200] [Batch 346/938] [D loss: 0.166878] [G loss: 1.213071]\n",
      "[Epoch 1/200] [Batch 347/938] [D loss: 0.179980] [G loss: 0.199634]\n",
      "[Epoch 1/200] [Batch 348/938] [D loss: 0.128606] [G loss: 0.866809]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_59108/2023726739.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     98\u001b[0m         \u001b[0md_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0md_real_loss\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0md_fake_loss\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m         \u001b[0md_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    101\u001b[0m         \u001b[0moptimizer_D\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\pt1\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 307\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\pt1\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 156\u001b[1;33m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    157\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.11 64-bit ('pt1': conda)"
  },
  "interpreter": {
   "hash": "51ae7e33fb81f5eb4ac0fc079093817bc6fab19a7fc91ff1a5d482a67033dba3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}